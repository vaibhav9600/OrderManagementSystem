* 
* ==> Audit <==
* |---------|----------------------------|----------|---------------|---------|---------------------|---------------------|
| Command |            Args            | Profile  |     User      | Version |     Start Time      |      End Time       |
|---------|----------------------------|----------|---------------|---------|---------------------|---------------------|
| start   |                            | minikube | vaibhavmishra | v1.31.2 | 31 Aug 23 11:12 IST | 31 Aug 23 11:31 IST |
| start   |                            | minikube | vaibhavmishra | v1.31.2 | 31 Aug 23 11:41 IST | 31 Aug 23 11:41 IST |
| ip      |                            | minikube | vaibhavmishra | v1.31.2 | 31 Aug 23 11:56 IST | 31 Aug 23 11:56 IST |
| ip      |                            | minikube | vaibhavmishra | v1.31.2 | 31 Aug 23 12:26 IST | 31 Aug 23 12:26 IST |
| ip      |                            | minikube | vaibhavmishra | v1.31.2 | 31 Aug 23 12:52 IST | 31 Aug 23 12:52 IST |
| ip      |                            | minikube | vaibhavmishra | v1.31.2 | 31 Aug 23 13:06 IST | 31 Aug 23 13:06 IST |
| ip      |                            | minikube | vaibhavmishra | v1.31.2 | 31 Aug 23 13:11 IST | 31 Aug 23 13:11 IST |
| start   |                            | minikube | vaibhavmishra | v1.31.2 | 31 Aug 23 14:56 IST | 31 Aug 23 14:56 IST |
| ip      |                            | minikube | vaibhavmishra | v1.31.2 | 31 Aug 23 15:05 IST | 31 Aug 23 15:05 IST |
| service | your-backend-service --url | minikube | vaibhavmishra | v1.31.2 | 31 Aug 23 15:12 IST |                     |
| service | your-backend-service --url | minikube | vaibhavmishra | v1.31.2 | 31 Aug 23 15:13 IST |                     |
| service | your-backend-service --url | minikube | vaibhavmishra | v1.31.2 | 31 Aug 23 15:14 IST |                     |
| service | your-backend-service --url | minikube | vaibhavmishra | v1.31.2 | 31 Aug 23 15:23 IST |                     |
| start   |                            | minikube | vaibhavmishra | v1.31.2 | 31 Aug 23 15:44 IST | 31 Aug 23 15:45 IST |
| service | your-backend-service --url | minikube | vaibhavmishra | v1.31.2 | 31 Aug 23 15:47 IST |                     |
|---------|----------------------------|----------|---------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/08/31 15:44:13
Running on machine: IM-CHN-LAP-0020
Binary: Built with gc go1.20.7 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0831 15:44:13.485260    1653 out.go:296] Setting OutFile to fd 1 ...
I0831 15:44:13.485898    1653 out.go:348] isatty.IsTerminal(1) = true
I0831 15:44:13.485900    1653 out.go:309] Setting ErrFile to fd 2...
I0831 15:44:13.485905    1653 out.go:348] isatty.IsTerminal(2) = true
I0831 15:44:13.487122    1653 root.go:338] Updating PATH: /Users/vaibhavmishra/.minikube/bin
W0831 15:44:13.487886    1653 root.go:314] Error reading config file at /Users/vaibhavmishra/.minikube/config/config.json: open /Users/vaibhavmishra/.minikube/config/config.json: no such file or directory
I0831 15:44:13.520117    1653 out.go:303] Setting JSON to false
I0831 15:44:13.548010    1653 start.go:128] hostinfo: {"hostname":"IM-CHN-LAP-0020.local","uptime":378,"bootTime":1693476475,"procs":519,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"13.5.1","kernelVersion":"22.6.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"d454a0b2-88c8-53a9-b770-a378edfb5ca3"}
W0831 15:44:13.548092    1653 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0831 15:44:13.554621    1653 out.go:177] üòÑ  minikube v1.31.2 on Darwin 13.5.1 (arm64)
I0831 15:44:13.564898    1653 notify.go:220] Checking for updates...
I0831 15:44:13.571839    1653 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I0831 15:44:13.572918    1653 driver.go:373] Setting default libvirt URI to qemu:///system
I0831 15:44:13.629235    1653 docker.go:121] docker version: linux-24.0.5:Docker Desktop 4.22.1 (118664)
I0831 15:44:13.629357    1653 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0831 15:44:13.945144    1653 info.go:266] docker info: {ID:189862b9-5ae4-41b1-bb7d-f6628571bc05 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:57 OomKillDisable:false NGoroutines:94 SystemTime:2023-08-31 10:14:13.933380761 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:24 KernelVersion:5.15.49-linuxkit-pr OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:5 MemTotal:8232828928 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/vaibhavmishra/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.1] map[Name:compose Path:/Users/vaibhavmishra/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.20.2-desktop.1] map[Name:dev Path:/Users/vaibhavmishra/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/Users/vaibhavmishra/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:/Users/vaibhavmishra/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.6] map[Name:sbom Path:/Users/vaibhavmishra/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/Users/vaibhavmishra/.docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/Users/vaibhavmishra/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:0.20.0]] Warnings:<nil>}}
I0831 15:44:13.954195    1653 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0831 15:44:13.958070    1653 start.go:298] selected driver: docker
I0831 15:44:13.958075    1653 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0831 15:44:13.958121    1653 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0831 15:44:13.958256    1653 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0831 15:44:14.054352    1653 info.go:266] docker info: {ID:189862b9-5ae4-41b1-bb7d-f6628571bc05 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:57 OomKillDisable:false NGoroutines:94 SystemTime:2023-08-31 10:14:14.041218261 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:24 KernelVersion:5.15.49-linuxkit-pr OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:5 MemTotal:8232828928 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/vaibhavmishra/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.1] map[Name:compose Path:/Users/vaibhavmishra/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.20.2-desktop.1] map[Name:dev Path:/Users/vaibhavmishra/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/Users/vaibhavmishra/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:/Users/vaibhavmishra/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.6] map[Name:sbom Path:/Users/vaibhavmishra/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/Users/vaibhavmishra/.docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/Users/vaibhavmishra/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:0.20.0]] Warnings:<nil>}}
I0831 15:44:14.060632    1653 cni.go:84] Creating CNI manager for ""
I0831 15:44:14.060660    1653 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0831 15:44:14.060689    1653 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0831 15:44:14.069114    1653 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0831 15:44:14.073118    1653 cache.go:122] Beginning downloading kic base image for docker with docker
I0831 15:44:14.082061    1653 out.go:177] üöú  Pulling base image ...
I0831 15:44:14.085191    1653 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I0831 15:44:14.085204    1653 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon
I0831 15:44:14.085256    1653 preload.go:148] Found local preload: /Users/vaibhavmishra/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-arm64.tar.lz4
I0831 15:44:14.085276    1653 cache.go:57] Caching tarball of preloaded images
I0831 15:44:14.086050    1653 preload.go:174] Found /Users/vaibhavmishra/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-arm64.tar.lz4 in cache, skipping download
I0831 15:44:14.086107    1653 cache.go:60] Finished verifying existence of preloaded tar for  v1.27.4 on docker
I0831 15:44:14.086305    1653 profile.go:148] Saving config to /Users/vaibhavmishra/.minikube/profiles/minikube/config.json ...
I0831 15:44:14.129763    1653 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 to local cache
I0831 15:44:14.130117    1653 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local cache directory
I0831 15:44:14.130346    1653 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local cache directory, skipping pull
I0831 15:44:14.130349    1653 image.go:105] gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 exists in cache, skipping pull
I0831 15:44:14.130368    1653 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 as a tarball
I0831 15:44:14.130370    1653 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 from local cache
I0831 15:44:25.316418    1653 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 from cached tarball
I0831 15:44:25.316557    1653 cache.go:195] Successfully downloaded all kic artifacts
I0831 15:44:25.316689    1653 start.go:365] acquiring machines lock for minikube: {Name:mk18b6a87ae49f6e0a17f69040ec0474f57c6e61 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0831 15:44:25.317910    1653 start.go:369] acquired machines lock for "minikube" in 1.179292ms
I0831 15:44:25.317988    1653 start.go:96] Skipping create...Using existing machine configuration
I0831 15:44:25.318020    1653 fix.go:54] fixHost starting: 
I0831 15:44:25.318807    1653 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0831 15:44:25.373453    1653 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0831 15:44:25.373501    1653 fix.go:102] recreateIfNeeded on minikube: state= err=unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0831 15:44:25.373514    1653 fix.go:107] machineExists: false. err=machine does not exist
I0831 15:44:25.384605    1653 out.go:177] ü§∑  docker "minikube" container is missing, will recreate.
I0831 15:44:25.390621    1653 delete.go:124] DEMOLISHING minikube ...
I0831 15:44:25.390746    1653 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0831 15:44:25.434203    1653 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
W0831 15:44:25.434242    1653 stop.go:75] unable to get state: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0831 15:44:25.434251    1653 delete.go:128] stophost failed (probably ok): ssh power off: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0831 15:44:25.434607    1653 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0831 15:44:25.477331    1653 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0831 15:44:25.477391    1653 delete.go:82] Unable to get host status for minikube, assuming it has already been deleted: state: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0831 15:44:25.477467    1653 cli_runner.go:164] Run: docker container inspect -f {{.Id}} minikube
W0831 15:44:25.518565    1653 cli_runner.go:211] docker container inspect -f {{.Id}} minikube returned with exit code 1
I0831 15:44:25.518584    1653 kic.go:367] could not find the container minikube to remove it. will try anyways
I0831 15:44:25.518660    1653 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0831 15:44:25.566471    1653 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
W0831 15:44:25.566521    1653 oci.go:84] error getting container status, will try to delete anyways: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0831 15:44:25.566600    1653 cli_runner.go:164] Run: docker exec --privileged -t minikube /bin/bash -c "sudo init 0"
W0831 15:44:25.615043    1653 cli_runner.go:211] docker exec --privileged -t minikube /bin/bash -c "sudo init 0" returned with exit code 1
I0831 15:44:25.615055    1653 oci.go:647] error shutdown minikube: docker exec --privileged -t minikube /bin/bash -c "sudo init 0": exit status 1
stdout:

stderr:
Error response from daemon: No such container: minikube
I0831 15:44:26.616504    1653 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0831 15:44:26.688029    1653 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0831 15:44:26.688051    1653 oci.go:659] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0831 15:44:26.688054    1653 oci.go:661] temporary error: container minikube status is  but expect it to be exited
I0831 15:44:26.688082    1653 retry.go:31] will retry after 446.4533ms: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0831 15:44:27.136093    1653 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0831 15:44:27.209636    1653 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0831 15:44:27.209678    1653 oci.go:659] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0831 15:44:27.209682    1653 oci.go:661] temporary error: container minikube status is  but expect it to be exited
I0831 15:44:27.209701    1653 retry.go:31] will retry after 974.785367ms: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0831 15:44:28.186102    1653 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0831 15:44:28.259658    1653 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0831 15:44:28.259694    1653 oci.go:659] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0831 15:44:28.259697    1653 oci.go:661] temporary error: container minikube status is  but expect it to be exited
I0831 15:44:28.259715    1653 retry.go:31] will retry after 948.669262ms: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0831 15:44:29.209570    1653 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0831 15:44:29.282041    1653 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0831 15:44:29.282075    1653 oci.go:659] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0831 15:44:29.282080    1653 oci.go:661] temporary error: container minikube status is  but expect it to be exited
I0831 15:44:29.282097    1653 retry.go:31] will retry after 2.257616608s: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0831 15:44:31.541528    1653 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0831 15:44:31.611342    1653 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0831 15:44:31.611374    1653 oci.go:659] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0831 15:44:31.611377    1653 oci.go:661] temporary error: container minikube status is  but expect it to be exited
I0831 15:44:31.611392    1653 retry.go:31] will retry after 2.075272447s: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0831 15:44:33.688480    1653 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0831 15:44:33.762346    1653 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0831 15:44:33.762385    1653 oci.go:659] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0831 15:44:33.762389    1653 oci.go:661] temporary error: container minikube status is  but expect it to be exited
I0831 15:44:33.762409    1653 retry.go:31] will retry after 2.016947354s: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0831 15:44:35.781219    1653 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0831 15:44:35.868358    1653 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0831 15:44:35.868399    1653 oci.go:659] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0831 15:44:35.868405    1653 oci.go:661] temporary error: container minikube status is  but expect it to be exited
I0831 15:44:35.868426    1653 retry.go:31] will retry after 4.586055013s: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0831 15:44:40.456447    1653 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0831 15:44:40.548321    1653 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0831 15:44:40.548356    1653 oci.go:659] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0831 15:44:40.548361    1653 oci.go:661] temporary error: container minikube status is  but expect it to be exited
I0831 15:44:40.548385    1653 oci.go:88] couldn't shut down minikube (might be okay): verify shutdown: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
 
I0831 15:44:40.548466    1653 cli_runner.go:164] Run: docker rm -f -v minikube
I0831 15:44:40.600154    1653 cli_runner.go:164] Run: docker container inspect -f {{.Id}} minikube
W0831 15:44:40.648982    1653 cli_runner.go:211] docker container inspect -f {{.Id}} minikube returned with exit code 1
I0831 15:44:40.649103    1653 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0831 15:44:40.702971    1653 cli_runner.go:164] Run: docker network rm minikube
I0831 15:44:40.827672    1653 fix.go:114] Sleeping 1 second for extra luck!
I0831 15:44:41.831007    1653 start.go:125] createHost starting for "" (driver="docker")
I0831 15:44:41.840831    1653 out.go:204] üî•  Creating docker container (CPUs=2, Memory=4000MB) ...
I0831 15:44:41.841315    1653 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0831 15:44:41.841372    1653 client.go:168] LocalClient.Create starting
I0831 15:44:41.843987    1653 main.go:141] libmachine: Reading certificate data from /Users/vaibhavmishra/.minikube/certs/ca.pem
I0831 15:44:41.855598    1653 main.go:141] libmachine: Decoding PEM data...
I0831 15:44:41.855669    1653 main.go:141] libmachine: Parsing certificate...
I0831 15:44:41.856241    1653 main.go:141] libmachine: Reading certificate data from /Users/vaibhavmishra/.minikube/certs/cert.pem
I0831 15:44:41.863954    1653 main.go:141] libmachine: Decoding PEM data...
I0831 15:44:41.864006    1653 main.go:141] libmachine: Parsing certificate...
I0831 15:44:41.866263    1653 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0831 15:44:41.948443    1653 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0831 15:44:41.948532    1653 network_create.go:281] running [docker network inspect minikube] to gather additional debugging logs...
I0831 15:44:41.948543    1653 cli_runner.go:164] Run: docker network inspect minikube
W0831 15:44:41.998439    1653 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0831 15:44:41.998454    1653 network_create.go:284] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0831 15:44:41.998462    1653 network_create.go:286] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0831 15:44:41.998539    1653 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0831 15:44:42.046504    1653 network.go:209] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0x14000cf0be0}
I0831 15:44:42.046539    1653 network_create.go:123] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 65535 ...
I0831 15:44:42.046605    1653 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=65535 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0831 15:44:42.134170    1653 network_create.go:107] docker network minikube 192.168.49.0/24 created
I0831 15:44:42.134195    1653 kic.go:117] calculated static IP "192.168.49.2" for the "minikube" container
I0831 15:44:42.134310    1653 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0831 15:44:42.180426    1653 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0831 15:44:42.225947    1653 oci.go:103] Successfully created a docker volume minikube
I0831 15:44:42.226044    1653 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 -d /var/lib
I0831 15:44:42.618727    1653 oci.go:107] Successfully prepared a docker volume minikube
I0831 15:44:42.618780    1653 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I0831 15:44:42.618799    1653 kic.go:190] Starting extracting preloaded images to volume ...
I0831 15:44:42.619054    1653 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /Users/vaibhavmishra/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 -I lz4 -xf /preloaded.tar -C /extractDir
I0831 15:44:44.947353    1653 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /Users/vaibhavmishra/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 -I lz4 -xf /preloaded.tar -C /extractDir: (2.328181417s)
I0831 15:44:44.947382    1653 kic.go:199] duration metric: took 2.328560 seconds to extract preloaded images to volume
I0831 15:44:44.947564    1653 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0831 15:44:45.180722    1653 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=4000mb --memory-swap=4000mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631
I0831 15:44:45.420100    1653 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0831 15:44:45.471386    1653 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0831 15:44:45.515232    1653 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0831 15:44:45.599179    1653 oci.go:144] the created container "minikube" has a running status.
I0831 15:44:45.599203    1653 kic.go:221] Creating ssh key for kic: /Users/vaibhavmishra/.minikube/machines/minikube/id_rsa...
I0831 15:44:45.695839    1653 kic_runner.go:191] docker (temp): /Users/vaibhavmishra/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0831 15:44:45.758295    1653 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0831 15:44:45.808692    1653 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0831 15:44:45.808708    1653 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0831 15:44:45.894523    1653 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0831 15:44:45.942051    1653 machine.go:88] provisioning docker machine ...
I0831 15:44:45.942098    1653 ubuntu.go:169] provisioning hostname "minikube"
I0831 15:44:45.942196    1653 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 15:44:45.988990    1653 main.go:141] libmachine: Using SSH client type: native
I0831 15:44:45.989797    1653 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104911760] 0x1049141c0 <nil>  [] 0s} 127.0.0.1 49890 <nil> <nil>}
I0831 15:44:45.989805    1653 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0831 15:44:46.120184    1653 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0831 15:44:46.120275    1653 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 15:44:46.164915    1653 main.go:141] libmachine: Using SSH client type: native
I0831 15:44:46.165256    1653 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104911760] 0x1049141c0 <nil>  [] 0s} 127.0.0.1 49890 <nil> <nil>}
I0831 15:44:46.165265    1653 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0831 15:44:46.278017    1653 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0831 15:44:46.278030    1653 ubuntu.go:175] set auth options {CertDir:/Users/vaibhavmishra/.minikube CaCertPath:/Users/vaibhavmishra/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/vaibhavmishra/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/vaibhavmishra/.minikube/machines/server.pem ServerKeyPath:/Users/vaibhavmishra/.minikube/machines/server-key.pem ClientKeyPath:/Users/vaibhavmishra/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/vaibhavmishra/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/vaibhavmishra/.minikube}
I0831 15:44:46.278044    1653 ubuntu.go:177] setting up certificates
I0831 15:44:46.278048    1653 provision.go:83] configureAuth start
I0831 15:44:46.278126    1653 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0831 15:44:46.321992    1653 provision.go:138] copyHostCerts
I0831 15:44:46.322137    1653 exec_runner.go:144] found /Users/vaibhavmishra/.minikube/ca.pem, removing ...
I0831 15:44:46.322147    1653 exec_runner.go:203] rm: /Users/vaibhavmishra/.minikube/ca.pem
I0831 15:44:46.322512    1653 exec_runner.go:151] cp: /Users/vaibhavmishra/.minikube/certs/ca.pem --> /Users/vaibhavmishra/.minikube/ca.pem (1099 bytes)
I0831 15:44:46.323550    1653 exec_runner.go:144] found /Users/vaibhavmishra/.minikube/cert.pem, removing ...
I0831 15:44:46.323552    1653 exec_runner.go:203] rm: /Users/vaibhavmishra/.minikube/cert.pem
I0831 15:44:46.323832    1653 exec_runner.go:151] cp: /Users/vaibhavmishra/.minikube/certs/cert.pem --> /Users/vaibhavmishra/.minikube/cert.pem (1139 bytes)
I0831 15:44:46.333334    1653 exec_runner.go:144] found /Users/vaibhavmishra/.minikube/key.pem, removing ...
I0831 15:44:46.333337    1653 exec_runner.go:203] rm: /Users/vaibhavmishra/.minikube/key.pem
I0831 15:44:46.333791    1653 exec_runner.go:151] cp: /Users/vaibhavmishra/.minikube/certs/key.pem --> /Users/vaibhavmishra/.minikube/key.pem (1675 bytes)
I0831 15:44:46.334809    1653 provision.go:112] generating server cert: /Users/vaibhavmishra/.minikube/machines/server.pem ca-key=/Users/vaibhavmishra/.minikube/certs/ca.pem private-key=/Users/vaibhavmishra/.minikube/certs/ca-key.pem org=vaibhavmishra.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0831 15:44:46.440045    1653 provision.go:172] copyRemoteCerts
I0831 15:44:46.440443    1653 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0831 15:44:46.440500    1653 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 15:44:46.497557    1653 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49890 SSHKeyPath:/Users/vaibhavmishra/.minikube/machines/minikube/id_rsa Username:docker}
I0831 15:44:46.588519    1653 ssh_runner.go:362] scp /Users/vaibhavmishra/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1099 bytes)
I0831 15:44:46.622373    1653 ssh_runner.go:362] scp /Users/vaibhavmishra/.minikube/machines/server.pem --> /etc/docker/server.pem (1220 bytes)
I0831 15:44:46.646079    1653 ssh_runner.go:362] scp /Users/vaibhavmishra/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0831 15:44:46.668974    1653 provision.go:86] duration metric: configureAuth took 390.914375ms
I0831 15:44:46.668984    1653 ubuntu.go:193] setting minikube options for container-runtime
I0831 15:44:46.670665    1653 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I0831 15:44:46.670752    1653 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 15:44:46.716821    1653 main.go:141] libmachine: Using SSH client type: native
I0831 15:44:46.717174    1653 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104911760] 0x1049141c0 <nil>  [] 0s} 127.0.0.1 49890 <nil> <nil>}
I0831 15:44:46.717187    1653 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0831 15:44:46.836390    1653 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0831 15:44:46.836420    1653 ubuntu.go:71] root file system type: overlay
I0831 15:44:46.836580    1653 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0831 15:44:46.836764    1653 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 15:44:46.891391    1653 main.go:141] libmachine: Using SSH client type: native
I0831 15:44:46.891784    1653 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104911760] 0x1049141c0 <nil>  [] 0s} 127.0.0.1 49890 <nil> <nil>}
I0831 15:44:46.891840    1653 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0831 15:44:47.016188    1653 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0831 15:44:47.016479    1653 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 15:44:47.074511    1653 main.go:141] libmachine: Using SSH client type: native
I0831 15:44:47.074851    1653 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104911760] 0x1049141c0 <nil>  [] 0s} 127.0.0.1 49890 <nil> <nil>}
I0831 15:44:47.074859    1653 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0831 15:44:47.724278    1653 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2023-07-07 14:51:01.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2023-08-31 10:14:47.013527012 +0000
@@ -1,30 +1,32 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +34,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0831 15:44:47.724310    1653 machine.go:91] provisioned docker machine in 1.782222375s
I0831 15:44:47.724324    1653 client.go:171] LocalClient.Create took 5.882899708s
I0831 15:44:47.724373    1653 start.go:167] duration metric: libmachine.API.Create for "minikube" took 5.883008458s
I0831 15:44:47.724383    1653 start.go:300] post-start starting for "minikube" (driver="docker")
I0831 15:44:47.724398    1653 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0831 15:44:47.724734    1653 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0831 15:44:47.724878    1653 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 15:44:47.792371    1653 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49890 SSHKeyPath:/Users/vaibhavmishra/.minikube/machines/minikube/id_rsa Username:docker}
I0831 15:44:47.881378    1653 ssh_runner.go:195] Run: cat /etc/os-release
I0831 15:44:47.885388    1653 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0831 15:44:47.885404    1653 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0831 15:44:47.885409    1653 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0831 15:44:47.885412    1653 info.go:137] Remote host: Ubuntu 22.04.2 LTS
I0831 15:44:47.885418    1653 filesync.go:126] Scanning /Users/vaibhavmishra/.minikube/addons for local assets ...
I0831 15:44:47.885850    1653 filesync.go:126] Scanning /Users/vaibhavmishra/.minikube/files for local assets ...
I0831 15:44:47.886112    1653 start.go:303] post-start completed in 161.717ms
I0831 15:44:47.887459    1653 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0831 15:44:47.931971    1653 profile.go:148] Saving config to /Users/vaibhavmishra/.minikube/profiles/minikube/config.json ...
I0831 15:44:47.933192    1653 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0831 15:44:47.933238    1653 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 15:44:47.977285    1653 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49890 SSHKeyPath:/Users/vaibhavmishra/.minikube/machines/minikube/id_rsa Username:docker}
I0831 15:44:48.061541    1653 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0831 15:44:48.066403    1653 start.go:128] duration metric: createHost completed in 6.235313s
I0831 15:44:48.066463    1653 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0831 15:44:48.111214    1653 fix.go:128] unexpected machine state, will restart: <nil>
I0831 15:44:48.111233    1653 machine.go:88] provisioning docker machine ...
I0831 15:44:48.111260    1653 ubuntu.go:169] provisioning hostname "minikube"
I0831 15:44:48.111376    1653 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 15:44:48.155687    1653 main.go:141] libmachine: Using SSH client type: native
I0831 15:44:48.156021    1653 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104911760] 0x1049141c0 <nil>  [] 0s} 127.0.0.1 49890 <nil> <nil>}
I0831 15:44:48.156026    1653 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0831 15:44:48.279895    1653 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0831 15:44:48.279994    1653 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 15:44:48.325031    1653 main.go:141] libmachine: Using SSH client type: native
I0831 15:44:48.325376    1653 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104911760] 0x1049141c0 <nil>  [] 0s} 127.0.0.1 49890 <nil> <nil>}
I0831 15:44:48.325384    1653 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0831 15:44:48.440074    1653 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0831 15:44:48.440091    1653 ubuntu.go:175] set auth options {CertDir:/Users/vaibhavmishra/.minikube CaCertPath:/Users/vaibhavmishra/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/vaibhavmishra/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/vaibhavmishra/.minikube/machines/server.pem ServerKeyPath:/Users/vaibhavmishra/.minikube/machines/server-key.pem ClientKeyPath:/Users/vaibhavmishra/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/vaibhavmishra/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/vaibhavmishra/.minikube}
I0831 15:44:48.440099    1653 ubuntu.go:177] setting up certificates
I0831 15:44:48.440103    1653 provision.go:83] configureAuth start
I0831 15:44:48.440196    1653 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0831 15:44:48.485065    1653 provision.go:138] copyHostCerts
I0831 15:44:48.485161    1653 exec_runner.go:144] found /Users/vaibhavmishra/.minikube/key.pem, removing ...
I0831 15:44:48.485165    1653 exec_runner.go:203] rm: /Users/vaibhavmishra/.minikube/key.pem
I0831 15:44:48.485498    1653 exec_runner.go:151] cp: /Users/vaibhavmishra/.minikube/certs/key.pem --> /Users/vaibhavmishra/.minikube/key.pem (1675 bytes)
I0831 15:44:48.486047    1653 exec_runner.go:144] found /Users/vaibhavmishra/.minikube/ca.pem, removing ...
I0831 15:44:48.491279    1653 exec_runner.go:203] rm: /Users/vaibhavmishra/.minikube/ca.pem
I0831 15:44:48.491594    1653 exec_runner.go:151] cp: /Users/vaibhavmishra/.minikube/certs/ca.pem --> /Users/vaibhavmishra/.minikube/ca.pem (1099 bytes)
I0831 15:44:48.492660    1653 exec_runner.go:144] found /Users/vaibhavmishra/.minikube/cert.pem, removing ...
I0831 15:44:48.492662    1653 exec_runner.go:203] rm: /Users/vaibhavmishra/.minikube/cert.pem
I0831 15:44:48.492856    1653 exec_runner.go:151] cp: /Users/vaibhavmishra/.minikube/certs/cert.pem --> /Users/vaibhavmishra/.minikube/cert.pem (1139 bytes)
I0831 15:44:48.493226    1653 provision.go:112] generating server cert: /Users/vaibhavmishra/.minikube/machines/server.pem ca-key=/Users/vaibhavmishra/.minikube/certs/ca.pem private-key=/Users/vaibhavmishra/.minikube/certs/ca-key.pem org=vaibhavmishra.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0831 15:44:48.614853    1653 provision.go:172] copyRemoteCerts
I0831 15:44:48.614920    1653 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0831 15:44:48.614985    1653 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 15:44:48.670058    1653 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49890 SSHKeyPath:/Users/vaibhavmishra/.minikube/machines/minikube/id_rsa Username:docker}
I0831 15:44:48.761757    1653 ssh_runner.go:362] scp /Users/vaibhavmishra/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1099 bytes)
I0831 15:44:48.782931    1653 ssh_runner.go:362] scp /Users/vaibhavmishra/.minikube/machines/server.pem --> /etc/docker/server.pem (1220 bytes)
I0831 15:44:48.804074    1653 ssh_runner.go:362] scp /Users/vaibhavmishra/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0831 15:44:48.825840    1653 provision.go:86] duration metric: configureAuth took 385.725708ms
I0831 15:44:48.825850    1653 ubuntu.go:193] setting minikube options for container-runtime
I0831 15:44:48.827626    1653 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I0831 15:44:48.827718    1653 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 15:44:48.881589    1653 main.go:141] libmachine: Using SSH client type: native
I0831 15:44:48.881930    1653 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104911760] 0x1049141c0 <nil>  [] 0s} 127.0.0.1 49890 <nil> <nil>}
I0831 15:44:48.881933    1653 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0831 15:44:49.000867    1653 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0831 15:44:49.000894    1653 ubuntu.go:71] root file system type: overlay
I0831 15:44:49.001159    1653 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0831 15:44:49.001404    1653 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 15:44:49.059829    1653 main.go:141] libmachine: Using SSH client type: native
I0831 15:44:49.060166    1653 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104911760] 0x1049141c0 <nil>  [] 0s} 127.0.0.1 49890 <nil> <nil>}
I0831 15:44:49.060209    1653 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0831 15:44:49.187310    1653 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0831 15:44:49.187742    1653 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 15:44:49.241980    1653 main.go:141] libmachine: Using SSH client type: native
I0831 15:44:49.242364    1653 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104911760] 0x1049141c0 <nil>  [] 0s} 127.0.0.1 49890 <nil> <nil>}
I0831 15:44:49.242373    1653 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0831 15:44:49.363365    1653 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0831 15:44:49.363380    1653 machine.go:91] provisioned docker machine in 1.252131292s
I0831 15:44:49.363389    1653 start.go:300] post-start starting for "minikube" (driver="docker")
I0831 15:44:49.363401    1653 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0831 15:44:49.363619    1653 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0831 15:44:49.363728    1653 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 15:44:49.414823    1653 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49890 SSHKeyPath:/Users/vaibhavmishra/.minikube/machines/minikube/id_rsa Username:docker}
I0831 15:44:49.500358    1653 ssh_runner.go:195] Run: cat /etc/os-release
I0831 15:44:49.504414    1653 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0831 15:44:49.504445    1653 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0831 15:44:49.504456    1653 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0831 15:44:49.504461    1653 info.go:137] Remote host: Ubuntu 22.04.2 LTS
I0831 15:44:49.504468    1653 filesync.go:126] Scanning /Users/vaibhavmishra/.minikube/addons for local assets ...
I0831 15:44:49.504748    1653 filesync.go:126] Scanning /Users/vaibhavmishra/.minikube/files for local assets ...
I0831 15:44:49.504865    1653 start.go:303] post-start completed in 141.467458ms
I0831 15:44:49.504938    1653 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0831 15:44:49.505010    1653 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 15:44:49.560543    1653 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49890 SSHKeyPath:/Users/vaibhavmishra/.minikube/machines/minikube/id_rsa Username:docker}
I0831 15:44:49.643635    1653 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0831 15:44:49.648661    1653 fix.go:56] fixHost completed within 24.330446708s
I0831 15:44:49.648674    1653 start.go:83] releasing machines lock for "minikube", held for 24.330562125s
I0831 15:44:49.648840    1653 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0831 15:44:49.704408    1653 ssh_runner.go:195] Run: cat /version.json
I0831 15:44:49.704480    1653 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 15:44:49.705033    1653 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0831 15:44:49.705366    1653 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 15:44:49.754160    1653 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49890 SSHKeyPath:/Users/vaibhavmishra/.minikube/machines/minikube/id_rsa Username:docker}
I0831 15:44:49.754207    1653 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49890 SSHKeyPath:/Users/vaibhavmishra/.minikube/machines/minikube/id_rsa Username:docker}
I0831 15:44:49.838942    1653 ssh_runner.go:195] Run: systemctl --version
I0831 15:44:50.042552    1653 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0831 15:44:50.048066    1653 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0831 15:44:50.070410    1653 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0831 15:44:50.070568    1653 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0831 15:44:50.094397    1653 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0831 15:44:50.094434    1653 start.go:466] detecting cgroup driver to use...
I0831 15:44:50.094460    1653 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0831 15:44:50.094874    1653 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0831 15:44:50.110453    1653 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0831 15:44:50.119826    1653 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0831 15:44:50.129146    1653 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0831 15:44:50.129285    1653 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0831 15:44:50.138942    1653 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0831 15:44:50.148219    1653 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0831 15:44:50.158031    1653 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0831 15:44:50.168493    1653 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0831 15:44:50.178035    1653 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0831 15:44:50.188086    1653 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0831 15:44:50.196089    1653 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0831 15:44:50.203858    1653 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0831 15:44:50.271407    1653 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0831 15:44:50.342899    1653 start.go:466] detecting cgroup driver to use...
I0831 15:44:50.342926    1653 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0831 15:44:50.343129    1653 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0831 15:44:50.355763    1653 cruntime.go:276] skipping containerd shutdown because we are bound to it
I0831 15:44:50.355968    1653 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0831 15:44:50.367895    1653 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0831 15:44:50.384843    1653 ssh_runner.go:195] Run: which cri-dockerd
I0831 15:44:50.390854    1653 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0831 15:44:50.399369    1653 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0831 15:44:50.418976    1653 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0831 15:44:50.498713    1653 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0831 15:44:50.578335    1653 docker.go:535] configuring docker to use "cgroupfs" as cgroup driver...
I0831 15:44:50.578375    1653 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (144 bytes)
I0831 15:44:50.595393    1653 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0831 15:44:50.658391    1653 ssh_runner.go:195] Run: sudo systemctl restart docker
I0831 15:44:50.913380    1653 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0831 15:44:50.979449    1653 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0831 15:44:51.046149    1653 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0831 15:44:51.112613    1653 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0831 15:44:51.178474    1653 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0831 15:44:51.190992    1653 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0831 15:44:51.261424    1653 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0831 15:44:51.332702    1653 start.go:513] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0831 15:44:51.334155    1653 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0831 15:44:51.339144    1653 start.go:534] Will wait 60s for crictl version
I0831 15:44:51.339248    1653 ssh_runner.go:195] Run: which crictl
I0831 15:44:51.343315    1653 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0831 15:44:51.393777    1653 start.go:550] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.4
RuntimeApiVersion:  v1
I0831 15:44:51.394008    1653 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0831 15:44:51.426674    1653 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0831 15:44:51.460626    1653 out.go:204] üê≥  Preparing Kubernetes v1.27.4 on Docker 24.0.4 ...
I0831 15:44:51.460849    1653 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0831 15:44:51.574178    1653 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0831 15:44:51.574366    1653 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0831 15:44:51.578911    1653 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0831 15:44:51.589716    1653 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0831 15:44:51.636006    1653 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I0831 15:44:51.636079    1653 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0831 15:44:51.654804    1653 docker.go:636] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0831 15:44:51.654815    1653 docker.go:566] Images already preloaded, skipping extraction
I0831 15:44:51.654934    1653 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0831 15:44:51.673099    1653 docker.go:636] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0831 15:44:51.673113    1653 cache_images.go:84] Images are preloaded, skipping loading
I0831 15:44:51.673207    1653 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0831 15:44:51.740489    1653 cni.go:84] Creating CNI manager for ""
I0831 15:44:51.740499    1653 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0831 15:44:51.740526    1653 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0831 15:44:51.740535    1653 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.27.4 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0831 15:44:51.740670    1653 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.27.4
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0831 15:44:51.740728    1653 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.27.4/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0831 15:44:51.740814    1653 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.27.4
I0831 15:44:51.749559    1653 binaries.go:44] Found k8s binaries, skipping transfer
I0831 15:44:51.749637    1653 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0831 15:44:51.757998    1653 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0831 15:44:51.774396    1653 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0831 15:44:51.790416    1653 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0831 15:44:51.807483    1653 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0831 15:44:51.811767    1653 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0831 15:44:51.822759    1653 certs.go:56] Setting up /Users/vaibhavmishra/.minikube/profiles/minikube for IP: 192.168.49.2
I0831 15:44:51.822776    1653 certs.go:190] acquiring lock for shared ca certs: {Name:mk9b4da1dcda94f285507aa44c4018f21d8d7e3d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0831 15:44:51.830529    1653 certs.go:199] skipping minikubeCA CA generation: /Users/vaibhavmishra/.minikube/ca.key
I0831 15:44:51.840879    1653 certs.go:199] skipping proxyClientCA CA generation: /Users/vaibhavmishra/.minikube/proxy-client-ca.key
I0831 15:44:51.841654    1653 certs.go:315] skipping minikube-user signed cert generation: /Users/vaibhavmishra/.minikube/profiles/minikube/client.key
I0831 15:44:51.852069    1653 certs.go:315] skipping minikube signed cert generation: /Users/vaibhavmishra/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0831 15:44:51.860849    1653 certs.go:315] skipping aggregator signed cert generation: /Users/vaibhavmishra/.minikube/profiles/minikube/proxy-client.key
I0831 15:44:51.869551    1653 certs.go:437] found cert: /Users/vaibhavmishra/.minikube/certs/Users/vaibhavmishra/.minikube/certs/ca-key.pem (1679 bytes)
I0831 15:44:51.869641    1653 certs.go:437] found cert: /Users/vaibhavmishra/.minikube/certs/Users/vaibhavmishra/.minikube/certs/ca.pem (1099 bytes)
I0831 15:44:51.869706    1653 certs.go:437] found cert: /Users/vaibhavmishra/.minikube/certs/Users/vaibhavmishra/.minikube/certs/cert.pem (1139 bytes)
I0831 15:44:51.869786    1653 certs.go:437] found cert: /Users/vaibhavmishra/.minikube/certs/Users/vaibhavmishra/.minikube/certs/key.pem (1675 bytes)
I0831 15:44:51.870944    1653 ssh_runner.go:362] scp /Users/vaibhavmishra/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0831 15:44:51.893821    1653 ssh_runner.go:362] scp /Users/vaibhavmishra/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0831 15:44:51.915297    1653 ssh_runner.go:362] scp /Users/vaibhavmishra/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0831 15:44:51.936381    1653 ssh_runner.go:362] scp /Users/vaibhavmishra/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0831 15:44:51.958128    1653 ssh_runner.go:362] scp /Users/vaibhavmishra/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0831 15:44:51.980631    1653 ssh_runner.go:362] scp /Users/vaibhavmishra/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0831 15:44:52.001488    1653 ssh_runner.go:362] scp /Users/vaibhavmishra/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0831 15:44:52.024029    1653 ssh_runner.go:362] scp /Users/vaibhavmishra/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0831 15:44:52.045762    1653 ssh_runner.go:362] scp /Users/vaibhavmishra/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0831 15:44:52.067300    1653 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0831 15:44:52.084656    1653 ssh_runner.go:195] Run: openssl version
I0831 15:44:52.091337    1653 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0831 15:44:52.100555    1653 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0831 15:44:52.104746    1653 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Aug 31 06:01 /usr/share/ca-certificates/minikubeCA.pem
I0831 15:44:52.104839    1653 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0831 15:44:52.112321    1653 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0831 15:44:52.123262    1653 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0831 15:44:52.128480    1653 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0831 15:44:52.135773    1653 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0831 15:44:52.142812    1653 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0831 15:44:52.149997    1653 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0831 15:44:52.157148    1653 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0831 15:44:52.164508    1653 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0831 15:44:52.172328    1653 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0831 15:44:52.172483    1653 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0831 15:44:52.189352    1653 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0831 15:44:52.197619    1653 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0831 15:44:52.197662    1653 kubeadm.go:636] restartCluster start
I0831 15:44:52.197727    1653 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0831 15:44:52.206055    1653 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0831 15:44:52.206173    1653 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0831 15:44:52.269887    1653 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:49620"
I0831 15:44:52.269902    1653 kubeconfig.go:135] verify returned: got: 127.0.0.1:49620, want: 127.0.0.1:49889
I0831 15:44:52.270232    1653 lock.go:35] WriteFile acquiring /Users/vaibhavmishra/.kube/config: {Name:mk54235c100eee238e7db78bd1bbcfd947fd87c9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0831 15:44:52.277737    1653 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0831 15:44:52.287489    1653 api_server.go:166] Checking apiserver status ...
I0831 15:44:52.287567    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 15:44:52.297626    1653 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 15:44:52.297635    1653 api_server.go:166] Checking apiserver status ...
I0831 15:44:52.297722    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 15:44:52.306623    1653 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 15:44:52.807750    1653 api_server.go:166] Checking apiserver status ...
I0831 15:44:52.808052    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 15:44:52.818775    1653 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 15:44:53.307683    1653 api_server.go:166] Checking apiserver status ...
I0831 15:44:53.307839    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 15:44:53.337722    1653 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 15:44:53.807784    1653 api_server.go:166] Checking apiserver status ...
I0831 15:44:53.808291    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 15:44:53.839508    1653 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 15:44:54.307224    1653 api_server.go:166] Checking apiserver status ...
I0831 15:44:54.307480    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 15:44:54.331728    1653 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 15:44:54.808046    1653 api_server.go:166] Checking apiserver status ...
I0831 15:44:54.808739    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 15:44:54.843270    1653 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 15:44:55.307727    1653 api_server.go:166] Checking apiserver status ...
I0831 15:44:55.307947    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 15:44:55.336740    1653 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 15:44:55.807899    1653 api_server.go:166] Checking apiserver status ...
I0831 15:44:55.808368    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 15:44:55.843482    1653 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 15:44:56.307917    1653 api_server.go:166] Checking apiserver status ...
I0831 15:44:56.308306    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 15:44:56.340227    1653 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 15:44:56.807049    1653 api_server.go:166] Checking apiserver status ...
I0831 15:44:56.807272    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 15:44:56.835501    1653 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 15:44:57.307731    1653 api_server.go:166] Checking apiserver status ...
I0831 15:44:57.307908    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 15:44:57.336785    1653 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 15:44:57.807828    1653 api_server.go:166] Checking apiserver status ...
I0831 15:44:57.808284    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 15:44:57.838887    1653 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 15:44:58.307312    1653 api_server.go:166] Checking apiserver status ...
I0831 15:44:58.307714    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 15:44:58.334121    1653 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 15:44:58.807746    1653 api_server.go:166] Checking apiserver status ...
I0831 15:44:58.807943    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 15:44:58.833454    1653 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 15:44:59.307885    1653 api_server.go:166] Checking apiserver status ...
I0831 15:44:59.308189    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 15:44:59.337345    1653 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 15:44:59.807301    1653 api_server.go:166] Checking apiserver status ...
I0831 15:44:59.807524    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 15:44:59.832554    1653 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 15:45:00.307291    1653 api_server.go:166] Checking apiserver status ...
I0831 15:45:00.307567    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 15:45:00.333732    1653 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 15:45:00.807853    1653 api_server.go:166] Checking apiserver status ...
I0831 15:45:00.808147    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 15:45:00.838055    1653 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 15:45:01.307092    1653 api_server.go:166] Checking apiserver status ...
I0831 15:45:01.307631    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 15:45:01.337133    1653 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 15:45:01.807768    1653 api_server.go:166] Checking apiserver status ...
I0831 15:45:01.807948    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 15:45:01.836694    1653 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0831 15:45:02.288835    1653 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0831 15:45:02.288889    1653 kubeadm.go:1128] stopping kube-system containers ...
I0831 15:45:02.289428    1653 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0831 15:45:02.342009    1653 docker.go:462] Stopping containers: [ae74e91a7271 47dbbcd9247a 00191262daa3 e7e9e282ce46 715ae7db60f7 cfd5ec3da45b 0d0bbccb598a 0b01279b9cd1 cece2601c22c 4f23f4fb059c abe059e2a124 5ead7c33ce50 3e5e3cbedb5e 4054db415e97 3caea98ffbfc 02a1ff8cd22c 2c5972f7c8d4 44386460d47e 14a268df8f39 eba2f91e4659 93433ea0dc80 bc8ceba0ac18 fe41de80ed75 d634bacdbace dc902a085404 42d00998607e c394d7c8a3a3 79c948dc1dd4]
I0831 15:45:02.342353    1653 ssh_runner.go:195] Run: docker stop ae74e91a7271 47dbbcd9247a 00191262daa3 e7e9e282ce46 715ae7db60f7 cfd5ec3da45b 0d0bbccb598a 0b01279b9cd1 cece2601c22c 4f23f4fb059c abe059e2a124 5ead7c33ce50 3e5e3cbedb5e 4054db415e97 3caea98ffbfc 02a1ff8cd22c 2c5972f7c8d4 44386460d47e 14a268df8f39 eba2f91e4659 93433ea0dc80 bc8ceba0ac18 fe41de80ed75 d634bacdbace dc902a085404 42d00998607e c394d7c8a3a3 79c948dc1dd4
I0831 15:45:02.366609    1653 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0831 15:45:02.380538    1653 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0831 15:45:02.388588    1653 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0831 15:45:02.388655    1653 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0831 15:45:02.396805    1653 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0831 15:45:02.396815    1653 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0831 15:45:02.447170    1653 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0831 15:45:03.010754    1653 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0831 15:45:03.127170    1653 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0831 15:45:03.170815    1653 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0831 15:45:03.213927    1653 api_server.go:52] waiting for apiserver process to appear ...
I0831 15:45:03.214175    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 15:45:03.224869    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 15:45:03.771910    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 15:45:04.271404    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 15:45:04.284554    1653 api_server.go:72] duration metric: took 1.070626458s to wait for apiserver process to appear ...
I0831 15:45:04.284579    1653 api_server.go:88] waiting for apiserver healthz status ...
I0831 15:45:04.284596    1653 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49889/healthz ...
I0831 15:45:05.778346    1653 api_server.go:279] https://127.0.0.1:49889/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0831 15:45:05.778370    1653 api_server.go:103] status: https://127.0.0.1:49889/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0831 15:45:05.778387    1653 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49889/healthz ...
I0831 15:45:05.792395    1653 api_server.go:279] https://127.0.0.1:49889/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0831 15:45:05.792415    1653 api_server.go:103] status: https://127.0.0.1:49889/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0831 15:45:06.292778    1653 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49889/healthz ...
I0831 15:45:06.297995    1653 api_server.go:279] https://127.0.0.1:49889/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0831 15:45:06.298013    1653 api_server.go:103] status: https://127.0.0.1:49889/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0831 15:45:06.793326    1653 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49889/healthz ...
I0831 15:45:06.800810    1653 api_server.go:279] https://127.0.0.1:49889/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0831 15:45:06.800836    1653 api_server.go:103] status: https://127.0.0.1:49889/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0831 15:45:07.292728    1653 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49889/healthz ...
I0831 15:45:07.300103    1653 api_server.go:279] https://127.0.0.1:49889/healthz returned 200:
ok
I0831 15:45:07.311955    1653 api_server.go:141] control plane version: v1.27.4
I0831 15:45:07.311973    1653 api_server.go:131] duration metric: took 3.027365s to wait for apiserver health ...
I0831 15:45:07.311981    1653 cni.go:84] Creating CNI manager for ""
I0831 15:45:07.311994    1653 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0831 15:45:07.317040    1653 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0831 15:45:07.321261    1653 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0831 15:45:07.371633    1653 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0831 15:45:07.389557    1653 system_pods.go:43] waiting for kube-system pods to appear ...
I0831 15:45:07.399322    1653 system_pods.go:59] 7 kube-system pods found
I0831 15:45:07.399352    1653 system_pods.go:61] "coredns-5d78c9869d-bsppm" [cd3a2cc0-c66f-4ce2-a7f6-2edadb781cf3] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0831 15:45:07.399374    1653 system_pods.go:61] "etcd-minikube" [50a37b8c-7fe6-40e5-8fed-6c37f49b4603] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0831 15:45:07.399388    1653 system_pods.go:61] "kube-apiserver-minikube" [5457fe49-987f-4c9f-b42e-92e153ae6dd1] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0831 15:45:07.399403    1653 system_pods.go:61] "kube-controller-manager-minikube" [13d331b6-aa2b-42eb-933a-4f9ada06b535] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0831 15:45:07.399416    1653 system_pods.go:61] "kube-proxy-lhxvg" [7f451026-2552-42ff-b97a-d18301407ba0] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0831 15:45:07.399435    1653 system_pods.go:61] "kube-scheduler-minikube" [71185698-4ce6-4a12-a7d2-a96234ada960] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0831 15:45:07.399449    1653 system_pods.go:61] "storage-provisioner" [0e661eeb-807d-4fce-9383-93bcc170b8dd] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0831 15:45:07.399457    1653 system_pods.go:74] duration metric: took 9.887958ms to wait for pod list to return data ...
I0831 15:45:07.399466    1653 node_conditions.go:102] verifying NodePressure condition ...
I0831 15:45:07.402678    1653 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0831 15:45:07.402695    1653 node_conditions.go:123] node cpu capacity is 5
I0831 15:45:07.402708    1653 node_conditions.go:105] duration metric: took 3.236208ms to run NodePressure ...
I0831 15:45:07.402727    1653 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0831 15:45:07.523631    1653 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0831 15:45:07.568095    1653 ops.go:34] apiserver oom_adj: -16
I0831 15:45:07.568108    1653 kubeadm.go:640] restartCluster took 15.370321042s
I0831 15:45:07.568115    1653 kubeadm.go:406] StartCluster complete in 15.395669791s
I0831 15:45:07.568132    1653 settings.go:142] acquiring lock: {Name:mk921d971fc8dc7aff5ce875b4be8eee88ad6545 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0831 15:45:07.568855    1653 settings.go:150] Updating kubeconfig:  /Users/vaibhavmishra/.kube/config
I0831 15:45:07.569601    1653 lock.go:35] WriteFile acquiring /Users/vaibhavmishra/.kube/config: {Name:mk54235c100eee238e7db78bd1bbcfd947fd87c9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0831 15:45:07.570282    1653 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0831 15:45:07.570413    1653 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false]
I0831 15:45:07.570540    1653 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0831 15:45:07.570552    1653 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0831 15:45:07.570557    1653 addons.go:240] addon storage-provisioner should already be in state true
I0831 15:45:07.570655    1653 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I0831 15:45:07.570662    1653 host.go:66] Checking if "minikube" exists ...
I0831 15:45:07.570676    1653 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0831 15:45:07.570686    1653 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0831 15:45:07.573241    1653 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0831 15:45:07.573406    1653 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0831 15:45:07.596020    1653 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0831 15:45:07.596042    1653 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}
I0831 15:45:07.600961    1653 out.go:177] üîé  Verifying Kubernetes components...
I0831 15:45:07.608057    1653 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0831 15:45:07.639862    1653 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0831 15:45:07.642963    1653 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0831 15:45:07.642969    1653 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0831 15:45:07.643053    1653 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 15:45:07.673320    1653 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0831 15:45:07.673341    1653 addons.go:240] addon default-storageclass should already be in state true
I0831 15:45:07.673356    1653 host.go:66] Checking if "minikube" exists ...
I0831 15:45:07.673669    1653 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0831 15:45:07.691473    1653 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49890 SSHKeyPath:/Users/vaibhavmishra/.minikube/machines/minikube/id_rsa Username:docker}
I0831 15:45:07.691904    1653 start.go:874] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0831 15:45:07.691914    1653 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0831 15:45:07.721398    1653 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0831 15:45:07.721411    1653 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0831 15:45:07.721481    1653 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 15:45:07.738409    1653 api_server.go:52] waiting for apiserver process to appear ...
I0831 15:45:07.738524    1653 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 15:45:07.750930    1653 api_server.go:72] duration metric: took 154.861125ms to wait for apiserver process to appear ...
I0831 15:45:07.750945    1653 api_server.go:88] waiting for apiserver healthz status ...
I0831 15:45:07.750970    1653 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49889/healthz ...
I0831 15:45:07.755882    1653 api_server.go:279] https://127.0.0.1:49889/healthz returned 200:
ok
I0831 15:45:07.757176    1653 api_server.go:141] control plane version: v1.27.4
I0831 15:45:07.757182    1653 api_server.go:131] duration metric: took 6.234583ms to wait for apiserver health ...
I0831 15:45:07.757186    1653 system_pods.go:43] waiting for kube-system pods to appear ...
I0831 15:45:07.761578    1653 system_pods.go:59] 7 kube-system pods found
I0831 15:45:07.761595    1653 system_pods.go:61] "coredns-5d78c9869d-bsppm" [cd3a2cc0-c66f-4ce2-a7f6-2edadb781cf3] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0831 15:45:07.761606    1653 system_pods.go:61] "etcd-minikube" [50a37b8c-7fe6-40e5-8fed-6c37f49b4603] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0831 15:45:07.761610    1653 system_pods.go:61] "kube-apiserver-minikube" [5457fe49-987f-4c9f-b42e-92e153ae6dd1] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0831 15:45:07.761613    1653 system_pods.go:61] "kube-controller-manager-minikube" [13d331b6-aa2b-42eb-933a-4f9ada06b535] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0831 15:45:07.761615    1653 system_pods.go:61] "kube-proxy-lhxvg" [7f451026-2552-42ff-b97a-d18301407ba0] Running
I0831 15:45:07.761618    1653 system_pods.go:61] "kube-scheduler-minikube" [71185698-4ce6-4a12-a7d2-a96234ada960] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0831 15:45:07.761620    1653 system_pods.go:61] "storage-provisioner" [0e661eeb-807d-4fce-9383-93bcc170b8dd] Running
I0831 15:45:07.761622    1653 system_pods.go:74] duration metric: took 4.434375ms to wait for pod list to return data ...
I0831 15:45:07.761627    1653 kubeadm.go:581] duration metric: took 165.565208ms to wait for : map[apiserver:true system_pods:true] ...
I0831 15:45:07.761634    1653 node_conditions.go:102] verifying NodePressure condition ...
I0831 15:45:07.765595    1653 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0831 15:45:07.765604    1653 node_conditions.go:123] node cpu capacity is 5
I0831 15:45:07.765612    1653 node_conditions.go:105] duration metric: took 3.975333ms to run NodePressure ...
I0831 15:45:07.765618    1653 start.go:228] waiting for startup goroutines ...
I0831 15:45:07.768450    1653 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49890 SSHKeyPath:/Users/vaibhavmishra/.minikube/machines/minikube/id_rsa Username:docker}
I0831 15:45:07.791274    1653 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0831 15:45:07.869129    1653 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0831 15:45:08.355953    1653 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0831 15:45:08.360004    1653 addons.go:502] enable addons completed in 789.581042ms: enabled=[storage-provisioner default-storageclass]
I0831 15:45:08.360042    1653 start.go:233] waiting for cluster config update ...
I0831 15:45:08.360062    1653 start.go:242] writing updated cluster config ...
I0831 15:45:08.362146    1653 ssh_runner.go:195] Run: rm -f paused
I0831 15:45:08.515252    1653 start.go:600] kubectl: 1.27.2, cluster: 1.27.4 (minor skew: 0)
I0831 15:45:08.519958    1653 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Aug 31 10:15:07 minikube cri-dockerd[1455]: time="2023-08-31T10:15:07Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"backend-deployment-66bf9b7b55-2bt9b_default\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Aug 31 10:15:07 minikube cri-dockerd[1455]: time="2023-08-31T10:15:07Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"oms-vaibhav-app-deployment-7fb45b8967-5ww97_default\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Aug 31 10:15:10 minikube dockerd[1203]: time="2023-08-31T10:15:10.035169592Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 31 10:15:10 minikube dockerd[1203]: time="2023-08-31T10:15:10.035386051Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 10:15:10 minikube dockerd[1203]: time="2023-08-31T10:15:10.770918884Z" level=info msg="ignoring event" container=f65677e4c7757716cce254bd249ec6eace7844de44796c7ff9e0a3085de251e4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 31 10:15:10 minikube cri-dockerd[1455]: time="2023-08-31T10:15:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bf63da37ae2aab5a486ff5d556de289657a5b6550eb8ccf5053f7f5285cb4125/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 31 10:15:10 minikube cri-dockerd[1455]: time="2023-08-31T10:15:10Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"your-backend-deployment-6cfd97f8fd-jdgcq_default\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Aug 31 10:15:11 minikube cri-dockerd[1455]: time="2023-08-31T10:15:11Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"your-backend-deployment-6cfd97f8fd-jdgcq_default\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Aug 31 10:15:11 minikube dockerd[1203]: time="2023-08-31T10:15:11.813453260Z" level=info msg="ignoring event" container=bf63da37ae2aab5a486ff5d556de289657a5b6550eb8ccf5053f7f5285cb4125 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 31 10:15:11 minikube cri-dockerd[1455]: time="2023-08-31T10:15:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4021401c8fd811239e490e1f3da4a21bf434a48285b818294a9b7a2994caf005/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 31 10:15:12 minikube dockerd[1203]: time="2023-08-31T10:15:12.957039844Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 31 10:15:12 minikube dockerd[1203]: time="2023-08-31T10:15:12.957081260Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 10:15:13 minikube dockerd[1203]: time="2023-08-31T10:15:13.831713678Z" level=info msg="ignoring event" container=82fef7977d60461c1317f8cdeb7b38dc537842077277e6808c50791ab5515a5d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 31 10:15:14 minikube cri-dockerd[1455]: time="2023-08-31T10:15:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d1093721dd5ba62a5a9f4f99883403561303a4d294b5a9e74ba493e5179924b9/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 31 10:15:15 minikube dockerd[1203]: time="2023-08-31T10:15:15.789181595Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 31 10:15:15 minikube dockerd[1203]: time="2023-08-31T10:15:15.789222262Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 10:15:15 minikube dockerd[1203]: time="2023-08-31T10:15:15.848713720Z" level=info msg="ignoring event" container=ae2823de1f61a5c90646a1582d14b6b805edab553111f93269067fd238dd44f2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 31 10:15:16 minikube cri-dockerd[1455]: time="2023-08-31T10:15:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/da5ddfc58c497695f98f75d400f2035777ac7c0cb3e18d33297a1decff4c2014/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 31 10:15:18 minikube dockerd[1203]: time="2023-08-31T10:15:18.691000388Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 31 10:15:18 minikube dockerd[1203]: time="2023-08-31T10:15:18.691030388Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 10:15:18 minikube dockerd[1203]: time="2023-08-31T10:15:18.881103472Z" level=info msg="ignoring event" container=27c73455f912829e2a266ba68880d39f8b86513552a01e9035e30a4def0a8490 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 31 10:15:19 minikube cri-dockerd[1455]: time="2023-08-31T10:15:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d49b39c6350f4e3687c0eb4b8dea179d32bcbcb3e3ba81ce707663cdbb4f6508/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 31 10:15:20 minikube dockerd[1203]: time="2023-08-31T10:15:20.392401500Z" level=info msg="ignoring event" container=6e1f3c829bd408a09ba4f2aa1dad83b015e406c655918a0b770c5be220e78494 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 31 10:15:20 minikube dockerd[1203]: time="2023-08-31T10:15:20.450286000Z" level=info msg="ignoring event" container=67bec9e80be935c575efe6cec601c0b4cff011f5ab3826db40076c46896ef8e5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 31 10:15:20 minikube cri-dockerd[1455]: time="2023-08-31T10:15:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/88396ace992132ba745738526127af7640bf48067b522aeffa76e525fdf10e5a/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 31 10:15:21 minikube dockerd[1203]: time="2023-08-31T10:15:21.621395375Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 31 10:15:21 minikube dockerd[1203]: time="2023-08-31T10:15:21.621454750Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 10:15:21 minikube dockerd[1203]: time="2023-08-31T10:15:21.981434375Z" level=info msg="ignoring event" container=7bfcb40f8def4fd2b2416b3c67d88535e7baefef5f9fae2ac7a6437e376b3c89 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 31 10:15:22 minikube cri-dockerd[1455]: time="2023-08-31T10:15:22Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8e73b4f0708dfb7c3d2ee439053ad4b2f74b773c5f910e85f8124bd508ee7c98/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 31 10:15:28 minikube dockerd[1203]: time="2023-08-31T10:15:28.281260545Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 31 10:15:28 minikube dockerd[1203]: time="2023-08-31T10:15:28.281402462Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 10:15:32 minikube dockerd[1203]: time="2023-08-31T10:15:32.212715089Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 31 10:15:32 minikube dockerd[1203]: time="2023-08-31T10:15:32.212855047Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 10:15:35 minikube dockerd[1203]: time="2023-08-31T10:15:35.307832715Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 31 10:15:35 minikube dockerd[1203]: time="2023-08-31T10:15:35.307875590Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 10:15:37 minikube dockerd[1203]: time="2023-08-31T10:15:37.243125174Z" level=info msg="ignoring event" container=d2b92d113979fea821699d8e19e1a55b39e0e81378faa89c1f305261b586c759 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 31 10:15:38 minikube dockerd[1203]: time="2023-08-31T10:15:38.216675717Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 31 10:15:38 minikube dockerd[1203]: time="2023-08-31T10:15:38.216843508Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 10:15:41 minikube dockerd[1203]: time="2023-08-31T10:15:41.273889885Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 31 10:15:41 minikube dockerd[1203]: time="2023-08-31T10:15:41.274039593Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 10:15:56 minikube dockerd[1203]: time="2023-08-31T10:15:56.544630044Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 31 10:15:56 minikube dockerd[1203]: time="2023-08-31T10:15:56.544768461Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 10:15:59 minikube dockerd[1203]: time="2023-08-31T10:15:59.852163713Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 31 10:15:59 minikube dockerd[1203]: time="2023-08-31T10:15:59.852386088Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 10:16:03 minikube dockerd[1203]: time="2023-08-31T10:16:03.234725172Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 31 10:16:03 minikube dockerd[1203]: time="2023-08-31T10:16:03.234867422Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 10:16:06 minikube dockerd[1203]: time="2023-08-31T10:16:06.264847716Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 31 10:16:06 minikube dockerd[1203]: time="2023-08-31T10:16:06.265157507Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 10:16:10 minikube dockerd[1203]: time="2023-08-31T10:16:10.671912426Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 31 10:16:10 minikube dockerd[1203]: time="2023-08-31T10:16:10.672110343Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 10:16:51 minikube dockerd[1203]: time="2023-08-31T10:16:51.123040667Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 31 10:16:51 minikube dockerd[1203]: time="2023-08-31T10:16:51.123184250Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 10:16:54 minikube dockerd[1203]: time="2023-08-31T10:16:54.677403877Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 31 10:16:54 minikube dockerd[1203]: time="2023-08-31T10:16:54.680420793Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 10:17:01 minikube dockerd[1203]: time="2023-08-31T10:17:01.772396380Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 31 10:17:01 minikube dockerd[1203]: time="2023-08-31T10:17:01.772619922Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 10:17:05 minikube dockerd[1203]: time="2023-08-31T10:17:05.927664965Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 31 10:17:05 minikube dockerd[1203]: time="2023-08-31T10:17:05.927876715Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 10:17:13 minikube dockerd[1203]: time="2023-08-31T10:17:13.068768219Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 31 10:17:13 minikube dockerd[1203]: time="2023-08-31T10:17:13.070019094Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
9681f4136f174       ba04bb24b9575       About a minute ago   Running             storage-provisioner       5                   52dd2496bdc4c       storage-provisioner
0b08951cf6d22       97e04611ad434       2 minutes ago        Running             coredns                   4                   88396ace99213       coredns-5d78c9869d-bsppm
6e1f3c829bd40       97e04611ad434       2 minutes ago        Exited              coredns                   3                   67bec9e80be93       coredns-5d78c9869d-bsppm
f0f5c70c16517       532e5a30e948f       2 minutes ago        Running             kube-proxy                3                   b3c311da59e50       kube-proxy-lhxvg
d2b92d113979f       ba04bb24b9575       2 minutes ago        Exited              storage-provisioner       4                   52dd2496bdc4c       storage-provisioner
ec27d2605c3a4       6eb63895cb67f       2 minutes ago        Running             kube-scheduler            3                   097a634b85bbd       kube-scheduler-minikube
bc20f43cd68b3       64aece92d6bde       2 minutes ago        Running             kube-apiserver            3                   e60be4de40cb0       kube-apiserver-minikube
49e80c82080cc       389f6f052cf83       2 minutes ago        Running             kube-controller-manager   3                   d57362e031418       kube-controller-manager-minikube
8c608383454c2       24bc64e911039       2 minutes ago        Running             etcd                      3                   e90eb6cf079b2       etcd-minikube
47dbbcd9247ad       24bc64e911039       50 minutes ago       Exited              etcd                      2                   5ead7c33ce50e       etcd-minikube
00191262daa34       389f6f052cf83       50 minutes ago       Exited              kube-controller-manager   2                   cece2601c22c3       kube-controller-manager-minikube
e7e9e282ce468       6eb63895cb67f       50 minutes ago       Exited              kube-scheduler            2                   4f23f4fb059cf       kube-scheduler-minikube
cfd5ec3da45bf       532e5a30e948f       50 minutes ago       Exited              kube-proxy                2                   abe059e2a1245       kube-proxy-lhxvg
0d0bbccb598a8       64aece92d6bde       50 minutes ago       Exited              kube-apiserver            2                   4054db415e973       kube-apiserver-minikube

* 
* ==> coredns [0b08951cf6d2] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/arm64, go1.20, 055b2c3
[INFO] 127.0.0.1:40637 - 26037 "HINFO IN 5273545534823206251.3864759540831621126. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.102063959s

* 
* ==> coredns [6e1f3c829bd4] <==
* [INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/arm64, go1.20, 055b2c3
[INFO] plugin/health: Going into lameduck mode for 5s
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: connect: network is unreachable
[INFO] 127.0.0.1:57080 - 50539 "HINFO IN 6692766901356302062.8176391442185530357. udp 57 false 512" - - 0 5.000067836s
[ERROR] plugin/errors: 2 6692766901356302062.8176391442185530357. HINFO: dial udp 192.168.65.254:53: connect: network is unreachable
[INFO] 127.0.0.1:42983 - 36854 "HINFO IN 6692766901356302062.8176391442185530357. udp 57 false 512" - - 0 5.000042502s
[ERROR] plugin/errors: 2 6692766901356302062.8176391442185530357. HINFO: dial udp 192.168.65.254:53: connect: network is unreachable

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=fd7ecd9c4599bef9f04c0986c4a0187f98a4396e
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_08_31T11_31_57_0700
                    minikube.k8s.io/version=v1.31.2
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 31 Aug 2023 06:01:55 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 31 Aug 2023 10:17:19 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 31 Aug 2023 10:15:05 +0000   Thu, 31 Aug 2023 06:01:54 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 31 Aug 2023 10:15:05 +0000   Thu, 31 Aug 2023 06:01:54 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 31 Aug 2023 10:15:05 +0000   Thu, 31 Aug 2023 06:01:54 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 31 Aug 2023 10:15:05 +0000   Thu, 31 Aug 2023 06:01:55 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                5
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8039872Ki
  pods:               110
Allocatable:
  cpu:                5
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8039872Ki
  pods:               110
System Info:
  Machine ID:                 6b4925eea08a4472979e10574830d0ca
  System UUID:                6b4925eea08a4472979e10574830d0ca
  Boot ID:                    f63deece-b89c-45d2-8565-696214cdfabd
  Kernel Version:             5.15.49-linuxkit-pr
  OS Image:                   Ubuntu 22.04.2 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://24.0.4
  Kubelet Version:            v1.27.4
  Kube-Proxy Version:         v1.27.4
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                           CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                           ------------  ----------  ---------------  -------------  ---
  default                     backend-deployment-66bf9b7b55-2bt9b            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17m
  default                     go-app-deployment-84bc496cfd-dcb7s             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17m
  default                     nginx-deployment-6bcc76f6c-2nzwq               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17m
  default                     oms-vaibhav-app-deployment-7fb45b8967-5ww97    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17m
  default                     your-backend-deployment-6cfd97f8fd-jdgcq       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17m
  kube-system                 coredns-5d78c9869d-bsppm                       100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     4h15m
  kube-system                 etcd-minikube                                  100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         4h15m
  kube-system                 kube-apiserver-minikube                        250m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h15m
  kube-system                 kube-controller-manager-minikube               200m (4%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h15m
  kube-system                 kube-proxy-lhxvg                               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h15m
  kube-system                 kube-scheduler-minikube                        100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h15m
  kube-system                 storage-provisioner                            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h15m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (15%!)(MISSING)  0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-32Mi     0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-64Ki     0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 50m                    kube-proxy       
  Normal  Starting                 2m18s                  kube-proxy       
  Normal  RegisteredNode           50m                    node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 2m22s                  kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  2m22s (x8 over 2m22s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    2m22s (x8 over 2m22s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     2m22s (x7 over 2m22s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  2m22s                  kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           2m7s                   node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Aug31 10:13] cacheinfo: Unable to detect cache hierarchy for CPU 0
[  +0.007353] the cryptoloop driver has been deprecated and will be removed in in Linux 5.16
[  +0.004790] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +1.958361] FAT-fs (loop0): utf8 is not a recommended IO charset for FAT filesystems, filesystem will be case sensitive!
[  +0.000324] FAT-fs (loop0): utf8 is not a recommended IO charset for FAT filesystems, filesystem will be case sensitive!
[  +0.024834] grpcfuse: loading out-of-tree module taints kernel.

* 
* ==> etcd [47dbbcd9247a] <==
* {"level":"info","ts":"2023-08-31T09:26:54.899Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2023-08-31T09:26:54.899Z","caller":"embed/etcd.go:124","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-08-31T09:26:54.900Z","caller":"embed/etcd.go:484","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-08-31T09:26:54.903Z","caller":"embed/etcd.go:132","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2023-08-31T09:26:54.903Z","caller":"embed/etcd.go:306","msg":"starting an etcd server","etcd-version":"3.5.7","git-sha":"215b53cf3","go-version":"go1.17.13","go-os":"linux","go-arch":"arm64","max-cpu-set":5,"max-cpu-available":5,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2023-08-31T09:26:54.905Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"1.965458ms"}
{"level":"info","ts":"2023-08-31T09:26:55.004Z","caller":"etcdserver/server.go:509","msg":"recovered v2 store from snapshot","snapshot-index":10001,"snapshot-size":"7.5 kB"}
{"level":"info","ts":"2023-08-31T09:26:55.005Z","caller":"etcdserver/server.go:522","msg":"recovered v3 backend from snapshot","backend-size-bytes":1712128,"backend-size":"1.7 MB","backend-size-in-use-bytes":872448,"backend-size-in-use":"872 kB"}
{"level":"info","ts":"2023-08-31T09:26:55.088Z","caller":"etcdserver/raft.go:529","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":10284}
{"level":"info","ts":"2023-08-31T09:26:55.088Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2023-08-31T09:26:55.089Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 3"}
{"level":"info","ts":"2023-08-31T09:26:55.089Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 3, commit: 10284, applied: 10001, lastindex: 10284, lastterm: 3]"}
{"level":"info","ts":"2023-08-31T09:26:55.089Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2023-08-31T09:26:55.089Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-08-31T09:26:55.089Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2023-08-31T09:26:55.090Z","caller":"auth/store.go:1234","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2023-08-31T09:26:55.090Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":8034}
{"level":"info","ts":"2023-08-31T09:26:55.093Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":8308}
{"level":"info","ts":"2023-08-31T09:26:55.094Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2023-08-31T09:26:55.095Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2023-08-31T09:26:55.095Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-08-31T09:26:55.095Z","caller":"etcdserver/server.go:845","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.7","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2023-08-31T09:26:55.095Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2023-08-31T09:26:55.095Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2023-08-31T09:26:55.095Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2023-08-31T09:26:55.095Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2023-08-31T09:26:55.096Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-08-31T09:26:55.096Z","caller":"embed/etcd.go:586","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-08-31T09:26:55.096Z","caller":"embed/etcd.go:558","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-08-31T09:26:55.096Z","caller":"embed/etcd.go:275","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2023-08-31T09:26:55.096Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2023-08-31T09:26:55.589Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 3"}
{"level":"info","ts":"2023-08-31T09:26:55.589Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 3"}
{"level":"info","ts":"2023-08-31T09:26:55.589Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2023-08-31T09:26:55.589Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 4"}
{"level":"info","ts":"2023-08-31T09:26:55.589Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2023-08-31T09:26:55.589Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 4"}
{"level":"info","ts":"2023-08-31T09:26:55.590Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 4"}
{"level":"info","ts":"2023-08-31T09:26:55.591Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2023-08-31T09:26:55.591Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-08-31T09:26:55.591Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2023-08-31T09:26:55.591Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2023-08-31T09:26:55.592Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2023-08-31T09:26:55.595Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-08-31T09:26:55.596Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2023-08-31T09:36:55.619Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8689}
{"level":"info","ts":"2023-08-31T09:36:55.624Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":8689,"took":"3.727625ms","hash":1793436032}
{"level":"info","ts":"2023-08-31T09:36:55.624Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1793436032,"revision":8689,"compact-revision":8034}
{"level":"info","ts":"2023-08-31T09:41:55.676Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8940}
{"level":"info","ts":"2023-08-31T09:41:55.681Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":8940,"took":"3.5475ms","hash":1586757642}
{"level":"info","ts":"2023-08-31T09:41:55.681Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1586757642,"revision":8940,"compact-revision":8689}
{"level":"info","ts":"2023-08-31T09:46:55.686Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9217}
{"level":"info","ts":"2023-08-31T09:46:55.688Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":9217,"took":"1.691958ms","hash":2986507458}
{"level":"info","ts":"2023-08-31T09:46:55.688Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2986507458,"revision":9217,"compact-revision":8940}
{"level":"info","ts":"2023-08-31T09:51:55.700Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9496}
{"level":"info","ts":"2023-08-31T09:51:55.703Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":9496,"took":"2.212916ms","hash":119828592}
{"level":"info","ts":"2023-08-31T09:51:55.703Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":119828592,"revision":9496,"compact-revision":9217}
{"level":"info","ts":"2023-08-31T09:56:55.719Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9751}
{"level":"info","ts":"2023-08-31T09:56:55.724Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":9751,"took":"3.532375ms","hash":1231510969}
{"level":"info","ts":"2023-08-31T09:56:55.724Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1231510969,"revision":9751,"compact-revision":9496}

* 
* ==> etcd [8c608383454c] <==
* {"level":"warn","ts":"2023-08-31T10:15:04.089Z","caller":"flags/flag.go:93","msg":"unrecognized environment variable","environment-variable":"ETCD_UNSUPPORTED_ARCH=arm64"}
{"level":"info","ts":"2023-08-31T10:15:04.093Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2023-08-31T10:15:04.093Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2023-08-31T10:15:04.096Z","caller":"embed/etcd.go:124","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-08-31T10:15:04.096Z","caller":"embed/etcd.go:484","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-08-31T10:15:04.097Z","caller":"embed/etcd.go:132","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2023-08-31T10:15:04.097Z","caller":"embed/etcd.go:306","msg":"starting an etcd server","etcd-version":"3.5.7","git-sha":"215b53cf3","go-version":"go1.17.13","go-os":"linux","go-arch":"arm64","max-cpu-set":5,"max-cpu-available":5,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2023-08-31T10:15:04.107Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"9.606208ms"}
{"level":"info","ts":"2023-08-31T10:15:04.213Z","caller":"etcdserver/server.go:509","msg":"recovered v2 store from snapshot","snapshot-index":10001,"snapshot-size":"7.5 kB"}
{"level":"info","ts":"2023-08-31T10:15:04.213Z","caller":"etcdserver/server.go:522","msg":"recovered v3 backend from snapshot","backend-size-bytes":2551808,"backend-size":"2.6 MB","backend-size-in-use-bytes":1945600,"backend-size-in-use":"1.9 MB"}
{"level":"info","ts":"2023-08-31T10:15:04.289Z","caller":"etcdserver/raft.go:529","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":12726}
{"level":"info","ts":"2023-08-31T10:15:04.289Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2023-08-31T10:15:04.289Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 4"}
{"level":"info","ts":"2023-08-31T10:15:04.289Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 4, commit: 12726, applied: 10001, lastindex: 12726, lastterm: 4]"}
{"level":"info","ts":"2023-08-31T10:15:04.289Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2023-08-31T10:15:04.289Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-08-31T10:15:04.289Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2023-08-31T10:15:04.290Z","caller":"auth/store.go:1234","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2023-08-31T10:15:04.291Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":9751}
{"level":"info","ts":"2023-08-31T10:15:04.294Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":10317}
{"level":"info","ts":"2023-08-31T10:15:04.295Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2023-08-31T10:15:04.295Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2023-08-31T10:15:04.296Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-08-31T10:15:04.296Z","caller":"etcdserver/server.go:845","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.7","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2023-08-31T10:15:04.296Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2023-08-31T10:15:04.297Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2023-08-31T10:15:04.297Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2023-08-31T10:15:04.297Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2023-08-31T10:15:04.304Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-08-31T10:15:04.305Z","caller":"embed/etcd.go:275","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2023-08-31T10:15:04.305Z","caller":"embed/etcd.go:586","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-08-31T10:15:04.305Z","caller":"embed/etcd.go:558","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-08-31T10:15:04.305Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2023-08-31T10:15:04.790Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 4"}
{"level":"info","ts":"2023-08-31T10:15:04.790Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 4"}
{"level":"info","ts":"2023-08-31T10:15:04.790Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2023-08-31T10:15:04.790Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 5"}
{"level":"info","ts":"2023-08-31T10:15:04.790Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 5"}
{"level":"info","ts":"2023-08-31T10:15:04.790Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 5"}
{"level":"info","ts":"2023-08-31T10:15:04.790Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 5"}
{"level":"info","ts":"2023-08-31T10:15:04.793Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2023-08-31T10:15:04.793Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-08-31T10:15:04.793Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-08-31T10:15:04.793Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2023-08-31T10:15:04.793Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2023-08-31T10:15:04.793Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2023-08-31T10:15:04.794Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"127.0.0.1:2379"}

* 
* ==> kernel <==
*  10:17:25 up 4 min,  0 users,  load average: 1.03, 0.96, 0.42
Linux minikube 5.15.49-linuxkit-pr #1 SMP PREEMPT Thu May 25 07:27:39 UTC 2023 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.2 LTS"

* 
* ==> kube-apiserver [0d0bbccb598a] <==
* W0831 09:26:56.015997       1 genericapiserver.go:752] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0831 09:26:56.023169       1 handler.go:232] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0831 09:26:56.023188       1 genericapiserver.go:752] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0831 09:26:56.404226       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0831 09:26:56.404254       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0831 09:26:56.404609       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0831 09:26:56.405005       1 secure_serving.go:210] Serving securely on [::]:8443
I0831 09:26:56.405082       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0831 09:26:56.405198       1 controller.go:83] Starting OpenAPI AggregationController
I0831 09:26:56.405374       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0831 09:26:56.405446       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0831 09:26:56.405459       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0831 09:26:56.405468       1 aggregator.go:150] waiting for initial CRD sync...
I0831 09:26:56.405606       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0831 09:26:56.405625       1 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0831 09:26:56.405694       1 apf_controller.go:361] Starting API Priority and Fairness config controller
I0831 09:26:56.405766       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0831 09:26:56.406677       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0831 09:26:56.406692       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0831 09:26:56.406712       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0831 09:26:56.406758       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0831 09:26:56.406794       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0831 09:26:56.407059       1 controller.go:121] Starting legacy_token_tracking_controller
I0831 09:26:56.407079       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I0831 09:26:56.407271       1 available_controller.go:423] Starting AvailableConditionController
I0831 09:26:56.407297       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0831 09:26:56.407328       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0831 09:26:56.407413       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0831 09:26:56.407610       1 handler_discovery.go:392] Starting ResourceDiscoveryManager
I0831 09:26:56.407703       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0831 09:26:56.408137       1 controller.go:85] Starting OpenAPI controller
I0831 09:26:56.408155       1 controller.go:85] Starting OpenAPI V3 controller
I0831 09:26:56.408186       1 naming_controller.go:291] Starting NamingConditionController
I0831 09:26:56.408193       1 establishing_controller.go:76] Starting EstablishingController
I0831 09:26:56.408218       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0831 09:26:56.408227       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0831 09:26:56.408232       1 crd_finalizer.go:266] Starting CRDFinalizer
I0831 09:26:56.491467       1 shared_informer.go:318] Caches are synced for node_authorizer
I0831 09:26:56.505805       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0831 09:26:56.505821       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0831 09:26:56.505811       1 apf_controller.go:366] Running API Priority and Fairness config worker
I0831 09:26:56.505830       1 apf_controller.go:369] Running API Priority and Fairness periodic rebalancing process
I0831 09:26:56.507467       1 shared_informer.go:318] Caches are synced for configmaps
I0831 09:26:56.507489       1 shared_informer.go:318] Caches are synced for crd-autoregister
I0831 09:26:56.507492       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0831 09:26:56.507514       1 aggregator.go:152] initial CRD sync complete...
I0831 09:26:56.507607       1 autoregister_controller.go:141] Starting autoregister controller
I0831 09:26:56.507616       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0831 09:26:56.507620       1 cache.go:39] Caches are synced for autoregister controller
I0831 09:26:56.532834       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0831 09:26:57.246707       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0831 09:26:57.417490       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0831 09:26:58.759188       1 controller.go:624] quota admission added evaluator for: endpoints
I0831 09:27:09.476272       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0831 09:28:00.803101       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0831 09:28:00.819367       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I0831 09:28:35.148679       1 alloc.go:330] "allocated clusterIPs" service="default/backend-deployment" clusterIPs=map[IPv4:10.99.107.18]
I0831 09:41:52.883061       1 alloc.go:330] "allocated clusterIPs" service="default/your-backend-service" clusterIPs=map[IPv4:10.97.245.250]
I0831 09:59:07.806845       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs=map[IPv4:10.96.0.1]
W0831 09:59:07.815864       1 lease.go:251] Resetting endpoints for master service "kubernetes" to [192.168.49.2]

* 
* ==> kube-apiserver [bc20f43cd68b] <==
* I0831 10:15:05.300461       1 handler.go:232] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0831 10:15:05.300480       1 genericapiserver.go:752] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0831 10:15:05.760849       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0831 10:15:05.760848       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0831 10:15:05.761213       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0831 10:15:05.761839       1 secure_serving.go:210] Serving securely on [::]:8443
I0831 10:15:05.761865       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0831 10:15:05.761914       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0831 10:15:05.761929       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0831 10:15:05.761969       1 apf_controller.go:361] Starting API Priority and Fairness config controller
I0831 10:15:05.761988       1 available_controller.go:423] Starting AvailableConditionController
I0831 10:15:05.761991       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0831 10:15:05.761998       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0831 10:15:05.762103       1 controller.go:83] Starting OpenAPI AggregationController
I0831 10:15:05.762139       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0831 10:15:05.762301       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0831 10:15:05.762347       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0831 10:15:05.762407       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0831 10:15:05.762547       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0831 10:15:05.762560       1 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0831 10:15:05.762573       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0831 10:15:05.762695       1 controller.go:121] Starting legacy_token_tracking_controller
I0831 10:15:05.762714       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I0831 10:15:05.762739       1 aggregator.go:150] waiting for initial CRD sync...
I0831 10:15:05.762978       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0831 10:15:05.762995       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0831 10:15:05.763030       1 controller.go:85] Starting OpenAPI controller
I0831 10:15:05.763057       1 controller.go:85] Starting OpenAPI V3 controller
I0831 10:15:05.763081       1 naming_controller.go:291] Starting NamingConditionController
I0831 10:15:05.763101       1 establishing_controller.go:76] Starting EstablishingController
I0831 10:15:05.763134       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0831 10:15:05.763151       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0831 10:15:05.763159       1 crd_finalizer.go:266] Starting CRDFinalizer
I0831 10:15:05.763180       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0831 10:15:05.763221       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0831 10:15:05.763951       1 handler_discovery.go:392] Starting ResourceDiscoveryManager
I0831 10:15:05.790006       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
E0831 10:15:05.795412       1 controller.go:155] Error removing old endpoints from kubernetes service: no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service
I0831 10:15:05.819973       1 shared_informer.go:318] Caches are synced for node_authorizer
I0831 10:15:05.862261       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0831 10:15:05.862327       1 apf_controller.go:366] Running API Priority and Fairness config worker
I0831 10:15:05.862333       1 apf_controller.go:369] Running API Priority and Fairness periodic rebalancing process
I0831 10:15:05.862390       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0831 10:15:05.862637       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0831 10:15:05.862848       1 shared_informer.go:318] Caches are synced for configmaps
I0831 10:15:05.864001       1 shared_informer.go:318] Caches are synced for crd-autoregister
I0831 10:15:05.864032       1 aggregator.go:152] initial CRD sync complete...
I0831 10:15:05.864071       1 autoregister_controller.go:141] Starting autoregister controller
I0831 10:15:05.864079       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0831 10:15:05.864087       1 cache.go:39] Caches are synced for autoregister controller
I0831 10:15:06.599733       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0831 10:15:06.772657       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0831 10:15:07.472383       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I0831 10:15:07.477775       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0831 10:15:07.500333       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0831 10:15:07.513283       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0831 10:15:07.516949       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0831 10:15:18.012154       1 controller.go:624] quota admission added evaluator for: endpoints
I0831 10:15:18.311210       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0831 10:17:01.489307       1 alloc.go:330] "allocated clusterIPs" service="default/your-backend-service" clusterIPs=map[IPv4:10.110.192.244]

* 
* ==> kube-controller-manager [00191262daa3] <==
* I0831 09:27:09.361574       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I0831 09:27:09.364308       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0831 09:27:09.366731       1 shared_informer.go:318] Caches are synced for crt configmap
I0831 09:27:09.370859       1 shared_informer.go:318] Caches are synced for deployment
I0831 09:27:09.373785       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0831 09:27:09.374483       1 shared_informer.go:318] Caches are synced for cronjob
I0831 09:27:09.374825       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0831 09:27:09.378459       1 shared_informer.go:318] Caches are synced for ReplicationController
I0831 09:27:09.389008       1 shared_informer.go:318] Caches are synced for GC
I0831 09:27:09.397533       1 shared_informer.go:318] Caches are synced for namespace
I0831 09:27:09.398851       1 shared_informer.go:318] Caches are synced for stateful set
I0831 09:27:09.399981       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0831 09:27:09.401230       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0831 09:27:09.402866       1 shared_informer.go:318] Caches are synced for persistent volume
I0831 09:27:09.402959       1 shared_informer.go:318] Caches are synced for TTL
I0831 09:27:09.404193       1 shared_informer.go:318] Caches are synced for expand
I0831 09:27:09.404331       1 shared_informer.go:318] Caches are synced for PVC protection
I0831 09:27:09.407917       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0831 09:27:09.407958       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0831 09:27:09.407979       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0831 09:27:09.407996       1 shared_informer.go:318] Caches are synced for ephemeral
I0831 09:27:09.409262       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0831 09:27:09.411850       1 shared_informer.go:318] Caches are synced for service account
I0831 09:27:09.445756       1 shared_informer.go:318] Caches are synced for TTL after finished
I0831 09:27:09.452059       1 shared_informer.go:318] Caches are synced for job
I0831 09:27:09.453397       1 shared_informer.go:318] Caches are synced for daemon sets
I0831 09:27:09.453420       1 shared_informer.go:318] Caches are synced for PV protection
I0831 09:27:09.453707       1 shared_informer.go:318] Caches are synced for taint
I0831 09:27:09.453856       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I0831 09:27:09.454025       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0831 09:27:09.454145       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I0831 09:27:09.454190       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0831 09:27:09.454257       1 taint_manager.go:211] "Sending events to api server"
I0831 09:27:09.455538       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0831 09:27:09.458233       1 shared_informer.go:318] Caches are synced for disruption
I0831 09:27:09.458339       1 shared_informer.go:318] Caches are synced for HPA
I0831 09:27:09.463381       1 shared_informer.go:318] Caches are synced for node
I0831 09:27:09.463485       1 range_allocator.go:174] "Sending events to api server"
I0831 09:27:09.463526       1 range_allocator.go:178] "Starting range CIDR allocator"
I0831 09:27:09.463538       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0831 09:27:09.463543       1 shared_informer.go:318] Caches are synced for cidrallocator
I0831 09:27:09.464946       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0831 09:27:09.468967       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0831 09:27:09.528963       1 shared_informer.go:318] Caches are synced for resource quota
I0831 09:27:09.561682       1 shared_informer.go:318] Caches are synced for resource quota
I0831 09:27:09.576989       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0831 09:27:09.587782       1 shared_informer.go:318] Caches are synced for attach detach
I0831 09:27:09.606360       1 shared_informer.go:318] Caches are synced for endpoint
I0831 09:27:09.976506       1 shared_informer.go:318] Caches are synced for garbage collector
I0831 09:27:09.983423       1 shared_informer.go:318] Caches are synced for garbage collector
I0831 09:27:09.983636       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0831 09:28:00.822434       1 event.go:307] "Event occurred" object="default/backend-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set backend-deployment-66bf9b7b55 to 1"
I0831 09:28:00.832551       1 event.go:307] "Event occurred" object="default/backend-deployment-66bf9b7b55" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: backend-deployment-66bf9b7b55-9vx5h"
I0831 09:41:36.883580       1 event.go:307] "Event occurred" object="default/your-backend-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set your-backend-deployment-6cfd97f8fd to 1"
I0831 09:41:36.887367       1 event.go:307] "Event occurred" object="default/your-backend-deployment-6cfd97f8fd" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: your-backend-deployment-6cfd97f8fd-f9zhf"
I0831 09:59:37.919076       1 event.go:307] "Event occurred" object="default/backend-deployment-66bf9b7b55" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: backend-deployment-66bf9b7b55-2bt9b"
I0831 09:59:37.921483       1 event.go:307] "Event occurred" object="default/go-app-deployment-84bc496cfd" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: go-app-deployment-84bc496cfd-dcb7s"
I0831 09:59:37.935521       1 event.go:307] "Event occurred" object="default/nginx-deployment-6bcc76f6c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: nginx-deployment-6bcc76f6c-2nzwq"
I0831 09:59:37.956482       1 event.go:307] "Event occurred" object="default/oms-vaibhav-app-deployment-7fb45b8967" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: oms-vaibhav-app-deployment-7fb45b8967-5ww97"
I0831 09:59:37.968955       1 event.go:307] "Event occurred" object="default/your-backend-deployment-6cfd97f8fd" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: your-backend-deployment-6cfd97f8fd-jdgcq"

* 
* ==> kube-controller-manager [49e80c82080c] <==
* I0831 10:15:17.898527       1 controllermanager.go:638] "Started controller" controller="persistentvolume-expander"
I0831 10:15:17.898604       1 expand_controller.go:339] "Starting expand controller"
I0831 10:15:17.898622       1 shared_informer.go:311] Waiting for caches to sync for expand
I0831 10:15:17.899962       1 controllermanager.go:638] "Started controller" controller="csrapproving"
I0831 10:15:17.899974       1 certificate_controller.go:112] Starting certificate controller "csrapproving"
I0831 10:15:17.899983       1 shared_informer.go:311] Waiting for caches to sync for certificate-csrapproving
I0831 10:15:17.908792       1 controllermanager.go:638] "Started controller" controller="ttl"
I0831 10:15:17.908897       1 ttl_controller.go:124] "Starting TTL controller"
I0831 10:15:17.908930       1 shared_informer.go:311] Waiting for caches to sync for TTL
I0831 10:15:17.911995       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I0831 10:15:17.922485       1 shared_informer.go:318] Caches are synced for PV protection
I0831 10:15:17.923990       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0831 10:15:17.935876       1 shared_informer.go:318] Caches are synced for endpoint
I0831 10:15:17.936942       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0831 10:15:17.974415       1 shared_informer.go:318] Caches are synced for TTL after finished
I0831 10:15:17.980759       1 shared_informer.go:318] Caches are synced for namespace
I0831 10:15:17.980772       1 shared_informer.go:318] Caches are synced for service account
I0831 10:15:17.983031       1 shared_informer.go:318] Caches are synced for job
I0831 10:15:17.985276       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0831 10:15:17.986544       1 shared_informer.go:318] Caches are synced for HPA
I0831 10:15:17.989130       1 shared_informer.go:318] Caches are synced for disruption
I0831 10:15:17.993518       1 shared_informer.go:318] Caches are synced for crt configmap
I0831 10:15:17.995842       1 shared_informer.go:318] Caches are synced for ReplicationController
I0831 10:15:17.997214       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0831 10:15:17.998620       1 shared_informer.go:318] Caches are synced for PVC protection
I0831 10:15:17.998643       1 shared_informer.go:318] Caches are synced for expand
I0831 10:15:17.999868       1 shared_informer.go:318] Caches are synced for ephemeral
I0831 10:15:18.001134       1 shared_informer.go:318] Caches are synced for deployment
I0831 10:15:18.002750       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0831 10:15:18.004087       1 shared_informer.go:318] Caches are synced for stateful set
I0831 10:15:18.012893       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0831 10:15:18.020551       1 shared_informer.go:318] Caches are synced for node
I0831 10:15:18.020589       1 range_allocator.go:174] "Sending events to api server"
I0831 10:15:18.020613       1 range_allocator.go:178] "Starting range CIDR allocator"
I0831 10:15:18.020630       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0831 10:15:18.020641       1 shared_informer.go:318] Caches are synced for cidrallocator
I0831 10:15:18.023264       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0831 10:15:18.043081       1 shared_informer.go:318] Caches are synced for attach detach
I0831 10:15:18.092339       1 shared_informer.go:318] Caches are synced for persistent volume
I0831 10:15:18.094596       1 shared_informer.go:318] Caches are synced for daemon sets
I0831 10:15:18.097962       1 shared_informer.go:318] Caches are synced for taint
I0831 10:15:18.098038       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I0831 10:15:18.098055       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0831 10:15:18.098076       1 taint_manager.go:211] "Sending events to api server"
I0831 10:15:18.098096       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0831 10:15:18.098130       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I0831 10:15:18.098228       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0831 10:15:18.100373       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0831 10:15:18.100439       1 shared_informer.go:318] Caches are synced for GC
I0831 10:15:18.109178       1 shared_informer.go:318] Caches are synced for TTL
I0831 10:15:18.128568       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0831 10:15:18.129763       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0831 10:15:18.129794       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0831 10:15:18.135000       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0831 10:15:18.192928       1 shared_informer.go:318] Caches are synced for resource quota
I0831 10:15:18.196642       1 shared_informer.go:318] Caches are synced for cronjob
I0831 10:15:18.212389       1 shared_informer.go:318] Caches are synced for resource quota
I0831 10:15:18.524371       1 shared_informer.go:318] Caches are synced for garbage collector
I0831 10:15:18.527869       1 shared_informer.go:318] Caches are synced for garbage collector
I0831 10:15:18.527888       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"

* 
* ==> kube-proxy [cfd5ec3da45b] <==
* E0831 09:26:54.722305       1 node.go:130] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused
E0831 09:26:56.479837       1 node.go:130] Failed to retrieve node info: nodes "minikube" is forbidden: User "system:serviceaccount:kube-system:kube-proxy" cannot get resource "nodes" in API group "" at the cluster scope
I0831 09:26:58.840847       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0831 09:26:58.840908       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I0831 09:26:58.840943       1 server_others.go:554] "Using iptables proxy"
I0831 09:26:58.860060       1 server_others.go:192] "Using iptables Proxier"
I0831 09:26:58.860082       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0831 09:26:58.860090       1 server_others.go:200] "Creating dualStackProxier for iptables"
I0831 09:26:58.860143       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I0831 09:26:58.860249       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0831 09:26:58.861234       1 server.go:658] "Version info" version="v1.27.4"
I0831 09:26:58.861253       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0831 09:26:58.861832       1 config.go:188] "Starting service config controller"
I0831 09:26:58.861923       1 config.go:315] "Starting node config controller"
I0831 09:26:58.861931       1 config.go:97] "Starting endpoint slice config controller"
I0831 09:26:58.861977       1 shared_informer.go:311] Waiting for caches to sync for node config
I0831 09:26:58.862002       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0831 09:26:58.862005       1 shared_informer.go:311] Waiting for caches to sync for service config
I0831 09:26:58.963493       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0831 09:26:58.963629       1 shared_informer.go:318] Caches are synced for service config
I0831 09:26:58.963758       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-proxy [f0f5c70c1651] <==
* I0831 10:15:07.299875       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0831 10:15:07.299977       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I0831 10:15:07.300003       1 server_others.go:554] "Using iptables proxy"
I0831 10:15:07.323098       1 server_others.go:192] "Using iptables Proxier"
I0831 10:15:07.323128       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0831 10:15:07.323133       1 server_others.go:200] "Creating dualStackProxier for iptables"
I0831 10:15:07.323144       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I0831 10:15:07.323232       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0831 10:15:07.323717       1 server.go:658] "Version info" version="v1.27.4"
I0831 10:15:07.323725       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0831 10:15:07.324418       1 config.go:188] "Starting service config controller"
I0831 10:15:07.324464       1 shared_informer.go:311] Waiting for caches to sync for service config
I0831 10:15:07.324508       1 config.go:315] "Starting node config controller"
I0831 10:15:07.324513       1 shared_informer.go:311] Waiting for caches to sync for node config
I0831 10:15:07.324765       1 config.go:97] "Starting endpoint slice config controller"
I0831 10:15:07.324771       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0831 10:15:07.425650       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0831 10:15:07.425668       1 shared_informer.go:318] Caches are synced for node config
I0831 10:15:07.425691       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-scheduler [e7e9e282ce46] <==
* I0831 09:26:55.348067       1 serving.go:348] Generated self-signed cert in-memory
W0831 09:26:56.430484       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0831 09:26:56.430559       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0831 09:26:56.430579       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0831 09:26:56.430622       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0831 09:26:56.477916       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.4"
I0831 09:26:56.478018       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0831 09:26:56.479490       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0831 09:26:56.479914       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0831 09:26:56.479573       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0831 09:26:56.480033       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0831 09:26:56.580381       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [ec27d2605c3a] <==
* I0831 10:15:04.721801       1 serving.go:348] Generated self-signed cert in-memory
I0831 10:15:05.800740       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.4"
I0831 10:15:05.800762       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0831 10:15:05.802878       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0831 10:15:05.802880       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0831 10:15:05.802894       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0831 10:15:05.802896       1 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0831 10:15:05.802880       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0831 10:15:05.803013       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0831 10:15:05.803247       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0831 10:15:05.803305       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0831 10:15:05.903670       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0831 10:15:05.903713       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0831 10:15:05.903671       1 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController

* 
* ==> kubelet <==
* Aug 31 10:15:55 minikube kubelet[1828]: E0831 10:15:55.293068    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-backend\\\"\"" pod="default/backend-deployment-66bf9b7b55-2bt9b" podUID=fba3710a-af32-40bf-8b76-e2960eb39c52
Aug 31 10:15:56 minikube kubelet[1828]: E0831 10:15:56.553537    1828 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for your-backend-image-name, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="your-backend-image-name:latest"
Aug 31 10:15:56 minikube kubelet[1828]: E0831 10:15:56.553689    1828 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for your-backend-image-name, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="your-backend-image-name:latest"
Aug 31 10:15:56 minikube kubelet[1828]: E0831 10:15:56.554127    1828 kuberuntime_manager.go:1212] container &Container{Name:your-backend-container,Image:your-backend-image-name,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5kvcb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod your-backend-deployment-6cfd97f8fd-jdgcq_default(2d44f97c-76d7-4cc2-b1bb-2891a4a4cfc8): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for your-backend-image-name, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 31 10:15:56 minikube kubelet[1828]: E0831 10:15:56.554183    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"your-backend-container\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for your-backend-image-name, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/your-backend-deployment-6cfd97f8fd-jdgcq" podUID=2d44f97c-76d7-4cc2-b1bb-2891a4a4cfc8
Aug 31 10:15:59 minikube kubelet[1828]: E0831 10:15:59.863512    1828 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-nginx:latest"
Aug 31 10:15:59 minikube kubelet[1828]: E0831 10:15:59.863657    1828 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-nginx:latest"
Aug 31 10:15:59 minikube kubelet[1828]: E0831 10:15:59.864351    1828 kuberuntime_manager.go:1212] container &Container{Name:nginx-container,Image:my-nginx,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ntv4j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod nginx-deployment-6bcc76f6c-2nzwq_default(da35c2dc-a86b-49a9-a4f8-d3dbf20a960a): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 31 10:15:59 minikube kubelet[1828]: E0831 10:15:59.864462    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nginx-container\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deployment-6bcc76f6c-2nzwq" podUID=da35c2dc-a86b-49a9-a4f8-d3dbf20a960a
Aug 31 10:16:03 minikube kubelet[1828]: E0831 10:16:03.244421    1828 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-go-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-go-app:latest"
Aug 31 10:16:03 minikube kubelet[1828]: E0831 10:16:03.244545    1828 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-go-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-go-app:latest"
Aug 31 10:16:03 minikube kubelet[1828]: E0831 10:16:03.244915    1828 kuberuntime_manager.go:1212] container &Container{Name:go-app-container,Image:my-go-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x6ntn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod go-app-deployment-84bc496cfd-dcb7s_default(75e81ebc-c4af-4da8-a60e-55629ffec4bd): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-go-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 31 10:16:03 minikube kubelet[1828]: E0831 10:16:03.244994    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app-container\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-go-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-app-deployment-84bc496cfd-dcb7s" podUID=75e81ebc-c4af-4da8-a60e-55629ffec4bd
Aug 31 10:16:06 minikube kubelet[1828]: E0831 10:16:06.275638    1828 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for oms-vaibhav-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="oms-vaibhav-app:latest"
Aug 31 10:16:06 minikube kubelet[1828]: E0831 10:16:06.275797    1828 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for oms-vaibhav-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="oms-vaibhav-app:latest"
Aug 31 10:16:06 minikube kubelet[1828]: E0831 10:16:06.276097    1828 kuberuntime_manager.go:1212] container &Container{Name:oms-vaibhav-app-container,Image:oms-vaibhav-app,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r4cbk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod oms-vaibhav-app-deployment-7fb45b8967-5ww97_default(25bb5c3d-b287-4b25-877a-71334bb51788): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for oms-vaibhav-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 31 10:16:06 minikube kubelet[1828]: E0831 10:16:06.276195    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"oms-vaibhav-app-container\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for oms-vaibhav-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/oms-vaibhav-app-deployment-7fb45b8967-5ww97" podUID=25bb5c3d-b287-4b25-877a-71334bb51788
Aug 31 10:16:10 minikube kubelet[1828]: E0831 10:16:10.684454    1828 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-backend:latest"
Aug 31 10:16:10 minikube kubelet[1828]: E0831 10:16:10.684661    1828 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-backend:latest"
Aug 31 10:16:10 minikube kubelet[1828]: E0831 10:16:10.684957    1828 kuberuntime_manager.go:1212] container &Container{Name:backend-container,Image:my-backend,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sb8tf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod backend-deployment-66bf9b7b55-2bt9b_default(fba3710a-af32-40bf-8b76-e2960eb39c52): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 31 10:16:10 minikube kubelet[1828]: E0831 10:16:10.685070    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend-container\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/backend-deployment-66bf9b7b55-2bt9b" podUID=fba3710a-af32-40bf-8b76-e2960eb39c52
Aug 31 10:16:11 minikube kubelet[1828]: E0831 10:16:11.280899    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nginx-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-nginx\\\"\"" pod="default/nginx-deployment-6bcc76f6c-2nzwq" podUID=da35c2dc-a86b-49a9-a4f8-d3dbf20a960a
Aug 31 10:16:11 minikube kubelet[1828]: E0831 10:16:11.280957    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"your-backend-container\" with ImagePullBackOff: \"Back-off pulling image \\\"your-backend-image-name\\\"\"" pod="default/your-backend-deployment-6cfd97f8fd-jdgcq" podUID=2d44f97c-76d7-4cc2-b1bb-2891a4a4cfc8
Aug 31 10:16:14 minikube kubelet[1828]: E0831 10:16:14.283625    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-go-app:latest\\\"\"" pod="default/go-app-deployment-84bc496cfd-dcb7s" podUID=75e81ebc-c4af-4da8-a60e-55629ffec4bd
Aug 31 10:16:21 minikube kubelet[1828]: E0831 10:16:21.291309    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"oms-vaibhav-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"oms-vaibhav-app\\\"\"" pod="default/oms-vaibhav-app-deployment-7fb45b8967-5ww97" podUID=25bb5c3d-b287-4b25-877a-71334bb51788
Aug 31 10:16:23 minikube kubelet[1828]: E0831 10:16:23.279116    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"your-backend-container\" with ImagePullBackOff: \"Back-off pulling image \\\"your-backend-image-name\\\"\"" pod="default/your-backend-deployment-6cfd97f8fd-jdgcq" podUID=2d44f97c-76d7-4cc2-b1bb-2891a4a4cfc8
Aug 31 10:16:24 minikube kubelet[1828]: E0831 10:16:24.292212    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-backend\\\"\"" pod="default/backend-deployment-66bf9b7b55-2bt9b" podUID=fba3710a-af32-40bf-8b76-e2960eb39c52
Aug 31 10:16:24 minikube kubelet[1828]: E0831 10:16:24.292275    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nginx-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-nginx\\\"\"" pod="default/nginx-deployment-6bcc76f6c-2nzwq" podUID=da35c2dc-a86b-49a9-a4f8-d3dbf20a960a
Aug 31 10:16:28 minikube kubelet[1828]: E0831 10:16:28.287866    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-go-app:latest\\\"\"" pod="default/go-app-deployment-84bc496cfd-dcb7s" podUID=75e81ebc-c4af-4da8-a60e-55629ffec4bd
Aug 31 10:16:32 minikube kubelet[1828]: E0831 10:16:32.279578    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"oms-vaibhav-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"oms-vaibhav-app\\\"\"" pod="default/oms-vaibhav-app-deployment-7fb45b8967-5ww97" podUID=25bb5c3d-b287-4b25-877a-71334bb51788
Aug 31 10:16:35 minikube kubelet[1828]: E0831 10:16:35.283354    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"your-backend-container\" with ImagePullBackOff: \"Back-off pulling image \\\"your-backend-image-name\\\"\"" pod="default/your-backend-deployment-6cfd97f8fd-jdgcq" podUID=2d44f97c-76d7-4cc2-b1bb-2891a4a4cfc8
Aug 31 10:16:37 minikube kubelet[1828]: E0831 10:16:37.289732    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-backend\\\"\"" pod="default/backend-deployment-66bf9b7b55-2bt9b" podUID=fba3710a-af32-40bf-8b76-e2960eb39c52
Aug 31 10:16:38 minikube kubelet[1828]: E0831 10:16:38.296252    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nginx-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-nginx\\\"\"" pod="default/nginx-deployment-6bcc76f6c-2nzwq" podUID=da35c2dc-a86b-49a9-a4f8-d3dbf20a960a
Aug 31 10:16:48 minikube kubelet[1828]: E0831 10:16:48.281489    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-backend\\\"\"" pod="default/backend-deployment-66bf9b7b55-2bt9b" podUID=fba3710a-af32-40bf-8b76-e2960eb39c52
Aug 31 10:16:51 minikube kubelet[1828]: E0831 10:16:51.137167    1828 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-go-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-go-app:latest"
Aug 31 10:16:51 minikube kubelet[1828]: E0831 10:16:51.137416    1828 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-go-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-go-app:latest"
Aug 31 10:16:51 minikube kubelet[1828]: E0831 10:16:51.139248    1828 kuberuntime_manager.go:1212] container &Container{Name:go-app-container,Image:my-go-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x6ntn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod go-app-deployment-84bc496cfd-dcb7s_default(75e81ebc-c4af-4da8-a60e-55629ffec4bd): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-go-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 31 10:16:51 minikube kubelet[1828]: E0831 10:16:51.139391    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app-container\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-go-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-app-deployment-84bc496cfd-dcb7s" podUID=75e81ebc-c4af-4da8-a60e-55629ffec4bd
Aug 31 10:16:54 minikube kubelet[1828]: E0831 10:16:54.697812    1828 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for oms-vaibhav-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="oms-vaibhav-app:latest"
Aug 31 10:16:54 minikube kubelet[1828]: E0831 10:16:54.697962    1828 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for oms-vaibhav-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="oms-vaibhav-app:latest"
Aug 31 10:16:54 minikube kubelet[1828]: E0831 10:16:54.698400    1828 kuberuntime_manager.go:1212] container &Container{Name:oms-vaibhav-app-container,Image:oms-vaibhav-app,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r4cbk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod oms-vaibhav-app-deployment-7fb45b8967-5ww97_default(25bb5c3d-b287-4b25-877a-71334bb51788): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for oms-vaibhav-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 31 10:16:54 minikube kubelet[1828]: E0831 10:16:54.698495    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"oms-vaibhav-app-container\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for oms-vaibhav-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/oms-vaibhav-app-deployment-7fb45b8967-5ww97" podUID=25bb5c3d-b287-4b25-877a-71334bb51788
Aug 31 10:17:01 minikube kubelet[1828]: E0831 10:17:01.783358    1828 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for your-backend-image-name, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="your-backend-image-name:latest"
Aug 31 10:17:01 minikube kubelet[1828]: E0831 10:17:01.783545    1828 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for your-backend-image-name, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="your-backend-image-name:latest"
Aug 31 10:17:01 minikube kubelet[1828]: E0831 10:17:01.784294    1828 kuberuntime_manager.go:1212] container &Container{Name:your-backend-container,Image:your-backend-image-name,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5kvcb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod your-backend-deployment-6cfd97f8fd-jdgcq_default(2d44f97c-76d7-4cc2-b1bb-2891a4a4cfc8): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for your-backend-image-name, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 31 10:17:01 minikube kubelet[1828]: E0831 10:17:01.784399    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"your-backend-container\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for your-backend-image-name, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/your-backend-deployment-6cfd97f8fd-jdgcq" podUID=2d44f97c-76d7-4cc2-b1bb-2891a4a4cfc8
Aug 31 10:17:05 minikube kubelet[1828]: E0831 10:17:05.291185    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-go-app:latest\\\"\"" pod="default/go-app-deployment-84bc496cfd-dcb7s" podUID=75e81ebc-c4af-4da8-a60e-55629ffec4bd
Aug 31 10:17:05 minikube kubelet[1828]: E0831 10:17:05.940434    1828 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-nginx:latest"
Aug 31 10:17:05 minikube kubelet[1828]: E0831 10:17:05.940549    1828 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-nginx:latest"
Aug 31 10:17:05 minikube kubelet[1828]: E0831 10:17:05.940832    1828 kuberuntime_manager.go:1212] container &Container{Name:nginx-container,Image:my-nginx,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ntv4j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod nginx-deployment-6bcc76f6c-2nzwq_default(da35c2dc-a86b-49a9-a4f8-d3dbf20a960a): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 31 10:17:05 minikube kubelet[1828]: E0831 10:17:05.940903    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nginx-container\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-nginx, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/nginx-deployment-6bcc76f6c-2nzwq" podUID=da35c2dc-a86b-49a9-a4f8-d3dbf20a960a
Aug 31 10:17:09 minikube kubelet[1828]: E0831 10:17:09.290996    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"oms-vaibhav-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"oms-vaibhav-app\\\"\"" pod="default/oms-vaibhav-app-deployment-7fb45b8967-5ww97" podUID=25bb5c3d-b287-4b25-877a-71334bb51788
Aug 31 10:17:13 minikube kubelet[1828]: E0831 10:17:13.076611    1828 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-backend:latest"
Aug 31 10:17:13 minikube kubelet[1828]: E0831 10:17:13.077279    1828 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-backend:latest"
Aug 31 10:17:13 minikube kubelet[1828]: E0831 10:17:13.077417    1828 kuberuntime_manager.go:1212] container &Container{Name:backend-container,Image:my-backend,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sb8tf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod backend-deployment-66bf9b7b55-2bt9b_default(fba3710a-af32-40bf-8b76-e2960eb39c52): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 31 10:17:13 minikube kubelet[1828]: E0831 10:17:13.077476    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend-container\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/backend-deployment-66bf9b7b55-2bt9b" podUID=fba3710a-af32-40bf-8b76-e2960eb39c52
Aug 31 10:17:17 minikube kubelet[1828]: E0831 10:17:17.288651    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"your-backend-container\" with ImagePullBackOff: \"Back-off pulling image \\\"your-backend-image-name\\\"\"" pod="default/your-backend-deployment-6cfd97f8fd-jdgcq" podUID=2d44f97c-76d7-4cc2-b1bb-2891a4a4cfc8
Aug 31 10:17:19 minikube kubelet[1828]: E0831 10:17:19.277097    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nginx-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-nginx\\\"\"" pod="default/nginx-deployment-6bcc76f6c-2nzwq" podUID=da35c2dc-a86b-49a9-a4f8-d3dbf20a960a
Aug 31 10:17:19 minikube kubelet[1828]: E0831 10:17:19.278785    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-go-app:latest\\\"\"" pod="default/go-app-deployment-84bc496cfd-dcb7s" podUID=75e81ebc-c4af-4da8-a60e-55629ffec4bd
Aug 31 10:17:21 minikube kubelet[1828]: E0831 10:17:21.292523    1828 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"oms-vaibhav-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"oms-vaibhav-app\\\"\"" pod="default/oms-vaibhav-app-deployment-7fb45b8967-5ww97" podUID=25bb5c3d-b287-4b25-877a-71334bb51788

* 
* ==> storage-provisioner [9681f4136f17] <==
* I0831 10:15:51.387508       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0831 10:15:51.393698       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0831 10:15:51.393784       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0831 10:16:08.831018       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0831 10:16:08.831330       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_3696adfe-0b2d-4f3e-a3f5-500626abcd58!
I0831 10:16:08.831942       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"283383cd-c254-400c-a9da-21b47969cc5d", APIVersion:"v1", ResourceVersion:"10568", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_3696adfe-0b2d-4f3e-a3f5-500626abcd58 became leader
I0831 10:16:08.931520       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_3696adfe-0b2d-4f3e-a3f5-500626abcd58!

* 
* ==> storage-provisioner [d2b92d113979] <==
* I0831 10:15:07.198325       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0831 10:15:37.209109       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

